{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"},{"sourceId":13906109,"sourceType":"datasetVersion","datasetId":8860034}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pickle\nimport joblib\nfrom pathlib import Path\nimport warnings\nimport os\nimport glob\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n# from catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error, mean_squared_log_error, mean_absolute_percentage_error, mean_tweedie_deviance\nfrom sklearn.model_selection import train_test_split\nimport kaggle_evaluation.nfl_inference_server\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:29.707857Z","iopub.execute_input":"2025-12-01T21:25:29.708576Z","iopub.status.idle":"2025-12-01T21:25:36.482200Z","shell.execute_reply.started":"2025-12-01T21:25:29.708549Z","shell.execute_reply":"2025-12-01T21:25:36.481422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    BASE_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n    CATBOOST_MODEL_PATH = \"Fall2025/AML/Project/models/catboost_5fold_models.pkl\"\n    LSTM_MODEL_DIR = \"Fall2025/AML/Project/models/output\"\n    \n    ENSEMBLE_WEIGHTS = { \n        'catboost': 0.5,\n        'lstm': 0.5\n    }\n    \n    ROLE_SPECIFIC_WEIGHTS = {\n        'Passer': {'catboost': 0.6, 'lstm': 0.4},\n        'Targeted Receiver': {'catboost': 0.4, 'lstm': 0.6},\n        'Defensive Coverage': {'catboost': 0.45, 'lstm': 0.55},\n        'default': {'catboost': 0.5, 'lstm': 0.5}\n    }\n    \n    USE_ROLE_SPECIFIC_WEIGHTS = False\n    \n    LSTM_N_FOLDS = 5\n    SEQ_LEN = 16\n    \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    MEAN_VECTOR = np.array([60.257755,26.618388,-0.048745,0.004431,-0.013931,-0.003644,179.821123])\n    STD_VECTOR = np.array([25.289261,13.446181,3.508402,3.879814,2.164451,2.220055,102.931777])\n'''\nmean\ntarget_x           60.257755\ntarget_y           26.618388\nvelocity_x         -0.048745\nvelocity_y          0.004431\nacceleration_x     -0.013931\nacceleration_y     -0.003644\ndir               179.821123\n\nstd\n\ntarget_x           25.289261\ntarget_y           13.446181\nvelocity_x          3.508402\nvelocity_y          3.879814\nacceleration_x      2.164451\nacceleration_y      2.220055\ndir               102.931777\n\n'''","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:36.483462Z","iopub.execute_input":"2025-12-01T21:25:36.483804Z","iopub.status.idle":"2025-12-01T21:25:36.491856Z","shell.execute_reply.started":"2025-12-01T21:25:36.483784Z","shell.execute_reply":"2025-12-01T21:25:36.491248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def height_to_feet(height_str):\n    try:\n        ft, inches = map(int, height_str.split('-'))\n        return ft + inches/12\n    except:\n        return None\n        \ndef engineer_features(df):\n    \"\"\"Create physics-based features for models\"\"\"\n    df = df.copy()\n    \n    df['velocity_x'] = df['s'] * np.cos(np.radians(df['dir']))\n    df['velocity_y'] = df['s'] * np.sin(np.radians(df['dir']))\n    \n    df['dist_to_ball'] = np.sqrt(\n        (df['x'] - df['ball_land_x'])**2 + \n        (df['y'] - df['ball_land_y'])**2\n    )\n    \n    df['angle_to_ball'] = np.arctan2(\n        df['ball_land_y'] - df['y'],\n        df['ball_land_x'] - df['x']\n    )\n    \n    df['velocity_toward_ball'] = (\n        df['velocity_x'] * np.cos(df['angle_to_ball']) + \n        df['velocity_y'] * np.sin(df['angle_to_ball'])\n    )\n    \n    df['time_to_ball'] = df['num_frames_output'] / 10.0\n    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n    \n    df['role_targeted_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n    df['role_defensive_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n    df['role_passer'] = (df['player_role'] == 'Passer').astype(int)\n    df['side_offense'] = (df['player_side'] == 'Offense').astype(int)\n    \n    height_parts = df['player_height'].str.split('-', expand=True)\n    df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703\n    \n    df['acceleration_x'] = df['a'] * np.cos(np.radians(df['dir']))\n    df['acceleration_y'] = df['a'] * np.sin(np.radians(df['dir']))\n    df['distance_to_target_x'] = df['ball_land_x'] - df['x']\n    df['distance_to_target_y'] = df['ball_land_y'] - df['y']\n    df['speed_squared'] = df['s'] ** 2\n    df['accel_magnitude'] = np.sqrt(df['acceleration_x']**2 + df['acceleration_y']**2)\n    df['velocity_alignment'] = np.cos(df['angle_to_ball'] - np.radians(df['dir']))\n    \n    df['expected_x_at_ball'] = df['x'] + df['velocity_x'] * df['time_to_ball']\n    df['expected_y_at_ball'] = df['y'] + df['velocity_y'] * df['time_to_ball']\n    df['error_from_ball_x'] = df['expected_x_at_ball'] - df['ball_land_x']\n    df['error_from_ball_y'] = df['expected_y_at_ball'] - df['ball_land_y']\n    df['error_from_ball'] = np.sqrt(df['error_from_ball_x']**2 + df['error_from_ball_y']**2)\n    \n    df['momentum_x'] = df['player_weight'] * df['velocity_x']\n    df['momentum_y'] = df['player_weight'] * df['velocity_y']\n    df['kinetic_energy'] = 0.5 * df['player_weight'] * df['speed_squared']\n    \n    df['angle_diff'] = np.abs(df['o'] - np.degrees(df['angle_to_ball']))\n    df['angle_diff'] = np.minimum(df['angle_diff'], 360 - df['angle_diff'])\n    \n    df['time_squared'] = df['time_to_ball'] ** 2\n    df['dist_squared'] = df['dist_to_ball'] ** 2\n    df['weighted_dist_by_time'] = df['dist_to_ball'] / (df['time_to_ball'] + 0.1)\n    \n    return df\n\ndef add_sequence_features_catboost(df):\n    \"\"\"Add temporal features using lag and rolling statistics\"\"\"\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    group_cols = ['game_id', 'play_id', 'nfl_id']\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            if col in df.columns:\n                df[f'{col}_rolling_mean_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n                df[f'{col}_rolling_std_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n    \n    for col in ['velocity_x', 'velocity_y']:\n        if col in df.columns:\n            df[f'{col}_delta'] = df.groupby(group_cols)[col].diff()\n    \n    return df\n\ndef create_training_dataset(input_df, output_df):\n    output_df = output_df.copy()\n    output_df['id'] = (output_df['game_id'].astype(str) + '_' + \n                    output_df['play_id'].astype(str) + '_' + \n                    output_df['nfl_id'].astype(str) + '_' + \n                    output_df['frame_id'].astype(str))\n    \n    output_df = output_df.rename(columns={'x': 'target_x', 'y': 'target_y'})\n    \n    input_agg = input_df.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n    \n    if 'frame_id' in input_agg.columns:\n        input_agg = input_agg.drop('frame_id', axis=1)\n    \n    merged = output_df.merge(\n        input_agg,\n        on=['game_id', 'play_id', 'nfl_id'],\n        how='left',\n        suffixes=('', '_input')\n    )\n    \n    return merged\n\ndef build_attention_sequences(input_df, output_df, seq_feature_cols, seq_len=8):\n    # Sort input by time\n    input_sorted = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    output_sorted = output_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    g_in = input_sorted.groupby(['game_id', 'play_id', 'nfl_id'])\n    g_out = output_sorted.groupby(['game_id', 'play_id', 'nfl_id'])\n    \n    common_keys = sorted(set(g_in.groups.keys()) & set(g_out.groups.keys()))\n    \n    sequences = []\n    times = []\n    targets = []\n    \n    for key in common_keys:\n        inp = g_in.get_group(key)\n        out = g_out.get_group(key)\n        \n        # base sequence of pre-throw frames\n        feat_mat = inp[seq_feature_cols].values  # (T_in, F)\n        if feat_mat.shape[0] >= seq_len:\n            base_seq = feat_mat[-seq_len:, :]\n        else:\n            # pad by repeating the first frame\n            pad = np.repeat(feat_mat[0:1, :], seq_len - feat_mat.shape[0], axis=0)\n            base_seq = np.concatenate([pad, feat_mat], axis=0)\n        \n        # same num_frames_output for this player\n        num_frames_output = inp['num_frames_output'].iloc[-1]\n        if num_frames_output <= 0:\n            continue\n        \n        for _, out_row in out.iterrows():\n            in_row = inp.iloc[-1]\n            \n            frame_id = out_row['frame_id']\n            t_norm = frame_id / num_frames_output  # normalized time in [0,1]\n            sequences.append(base_seq)\n            times.append(t_norm)\n            target = np.array([\n                out_row['x'], out_row['y'],\n                in_row['velocity_x'], in_row['velocity_y'],\n                in_row['acceleration_x'], in_row['acceleration_y'],\n                in_row['dir']\n            ], dtype=np.float32)\n            targets.append(target)\n    \n    sequences = np.stack(sequences)        # (N, seq_len, F)\n    times = np.array(times, dtype=np.float32)   # (N,)\n    targets = np.stack(targets)            # (N, 6)\n    \n    print(f\"Built {len(sequences)} sequence examples for attention model.\")\n    return sequences, times, targets\n\ndef read_files(file_path):\n    files = glob.glob(file_path)\n    df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n    return files, df\n\ndef initiate_feature_cols():\n    seq_feature_cols = [\n        'x', 'y', 's', 'a', 'o', 'dir',\n        'velocity_x', 'velocity_y', 'dist_to_ball', 'angle_to_ball',\n        'velocity_toward_ball', 'time_to_ball', 'orientation_diff',\n        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer',\n        'side_offense', 'height_inches', 'player_weight', 'bmi',\n        'ball_land_x', 'ball_land_y',\n        'acceleration_x', 'acceleration_y',\n        'distance_to_target_x', 'distance_to_target_y',\n        'speed_squared', 'accel_magnitude', 'velocity_alignment',\n        'expected_x_at_ball', 'expected_y_at_ball',\n        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        'angle_diff', 'time_squared', 'dist_squared', 'weighted_dist_by_time'\n    ]\n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            seq_feature_cols.append(f'{col}_lag{lag}')\n        \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            seq_feature_cols.append(f'{col}_rolling_mean_{window}')\n            seq_feature_cols.append(f'{col}_rolling_std_{window}')\n    \n    seq_feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n    return seq_feature_cols","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:36.492590Z","iopub.execute_input":"2025-12-01T21:25:36.492809Z","iopub.status.idle":"2025-12-01T21:25:36.517745Z","shell.execute_reply.started":"2025-12-01T21:25:36.492793Z","shell.execute_reply":"2025-12-01T21:25:36.517020Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AttentionNFLDataset(Dataset):\n    def __init__(self, sequences, times, targets):\n        self.sequences = sequences\n        self.times = times\n        self.targets = (targets - Config.MEAN_VECTOR) / Config.STD_VECTOR\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        x_seq = torch.tensor(self.sequences[idx], dtype=torch.float32)  # (T, F)\n        t = torch.tensor(self.times[idx], dtype=torch.float32)          # scalar\n        y = torch.tensor(self.targets[idx], dtype=torch.float32)        # (2,)\n        return x_seq, t, y","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:36.519029Z","iopub.execute_input":"2025-12-01T21:25:36.519426Z","iopub.status.idle":"2025-12-01T21:25:36.534011Z","shell.execute_reply.started":"2025-12-01T21:25:36.519406Z","shell.execute_reply":"2025-12-01T21:25:36.533363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=100):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n\n    def forward(self, x):\n        # x: (B, T, d_model)\n        T = x.size(1)\n        return x + self.pe[:, :T, :]\n\n\nclass AttentionNFL(nn.Module):\n    def __init__(self, n_features, d_model=128, n_heads=4, num_layers=3, seq_len=8):\n        super().__init__()\n        self.d_model = d_model\n        self.input_norm = nn.LayerNorm(n_features)\n        self.layer_norm = nn.LayerNorm(d_model)\n        # project raw features up to model dimension\n        \n        self.input_projection = nn.Linear(n_features, d_model)\n        self.pre_mlp = nn.Sequential(\n            nn.Linear(n_features, d_model),\n            nn.LeakyReLU(0.01),\n            nn.Linear(d_model, d_model),\n            nn.LeakyReLU(0.01),\n        )\n        \n        # self.pos_encoding = PositionalEncoding(d_model)\n        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=4*d_model,\n            dropout=0.1,\n            batch_first=True,\n            activation = 'gelu'\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.gru = nn.GRU(d_model, d_model, batch_first=True)\n        # time conditioning: t_norm ∈ [0,1] → embedding of size d_model\n        self.time_mlp = nn.Sequential(\n            nn.Linear(1, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, d_model),\n            nn.ReLU()\n        )\n        \n        # final head: [context; time_embed] → x,y\n        self.pos_head = nn.Sequential(\n            nn.Linear(2*d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, 2)\n        )     # predict (x, y)\n        self.vel_head = nn.Sequential(\n            nn.Linear(2*d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, 2)\n        )    # predict velocity vector\n        \n        self.acc_head = nn.Sequential(\n            nn.Linear(2*d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, 2)\n        )     # predict acceleration vector\n        \n        self.dir_head = nn.Sequential(\n            nn.Linear(2*d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, 1)\n        )      # predict facing direction\n\n    def forward(self, x_seq, t_norm):\n        \"\"\"\n        x_seq: (B, T, n_features)\n        t_norm: (B,)  normalized frame_id / num_frames_output\n        returns: (B, 2)  predicted (x, y)\n        \"\"\"\n        # encode sequence\n        T = x_seq.shape[1]\n        x_seq = self.input_norm(x_seq)\n        x = self.pre_mlp(x_seq)                           # (B, T, d_model)\n        x = x + self.pos_embedding[:, :T, :]              # (B, T, d_model)\n        # x = self.pos_encoding(x)                           # (B, T, d_model)\n        x = self.layer_norm(x)\n        enc = self.transformer(x)                         # (B, T, d_model)\n        enc, _ = self.gru(enc)\n        # use final timestep as context\n        context = enc[:, -1, :]                           # (B, d_model)\n        \n        # embed time\n        t_norm = t_norm.view(-1, 1)                       # (B, 1)\n        t_embed = self.time_mlp(t_norm)                   # (B, d_model)\n        \n        # combine and predict\n        h = torch.cat([context, t_embed], dim=-1)         # (B, 2*d_model)\n        pos_out = self.pos_head(h)                        # (B, 2)\n        vel_out = self.vel_head(h)                        # (B, 2)\n        acc_out = self.acc_head(h)                        # (B, 2)\n        dir_out = self.dir_head(h)                        # (B, 1)\n        return {\n            'pos': pos_out,\n            'vel': vel_out,\n            'acc': acc_out,\n            'dir': dir_out\n        }","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:36.535010Z","iopub.execute_input":"2025-12-01T21:25:36.535244Z","iopub.status.idle":"2025-12-01T21:25:36.552194Z","shell.execute_reply.started":"2025-12-01T21:25:36.535229Z","shell.execute_reply":"2025-12-01T21:25:36.551498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PhysicsWeightedMSELoss(nn.Module):\n    def __init__(\n        self,\n        w_pos=1.0,     # weight for position\n        w_vel=0.3,     # weight for velocity\n        w_acc=0.2,    # weight for acceleration\n        w_dir=0.1      # weight for direction\n    ):\n        super().__init__()\n        self.w_pos = w_pos\n        self.w_vel = w_vel\n        self.w_acc = w_acc\n        self.w_dir = w_dir\n        self.mse = nn.MSELoss()\n\n    def forward(self, y_pred, y_true):\n        \"\"\"\n        y_pred: (B, 7)\n        y_true: (B, 7)\n        \"\"\"\n\n        # Position terms\n        pos_pred = y_pred[:, 0:2]\n        pos_true = y_true[:, 0:2]\n        loss_pos = self.mse(pos_pred, pos_true)\n\n        # Velocity terms\n        vel_pred = y_pred[:, 2:4]\n        vel_true = y_true[:, 2:4]\n        loss_vel = self.mse(vel_pred, vel_true)\n\n        # Acceleration terms\n        acc_pred = y_pred[:, 4:6]\n        acc_true = y_true[:, 4:6]\n        loss_acc = self.mse(acc_pred, acc_true)\n\n        # Direction (scalar)\n        dir_pred = y_pred[:, 6]\n        dir_true = y_true[:, 6]\n        loss_dir = self.mse(dir_pred, dir_true)\n\n        # Weighted sum\n        total_loss = (self.w_pos * loss_pos +\n            self.w_vel * loss_vel +\n            self.w_acc * loss_acc +\n            self.w_dir * loss_dir\n        )\n\n        return total_loss, {\n            \"loss_pos\": loss_pos.item(),\n            \"loss_vel\": loss_vel.item(),\n            \"loss_acc\": loss_acc.item(),\n            \"loss_dir\": loss_dir.item(),\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:36.552822Z","iopub.execute_input":"2025-12-01T21:25:36.553068Z","iopub.status.idle":"2025-12-01T21:25:36.570730Z","shell.execute_reply.started":"2025-12-01T21:25:36.553052Z","shell.execute_reply":"2025-12-01T21:25:36.570246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_attention_model(\n    model,\n    train_loader,\n    val_loader,\n    epochs=10,\n    base_lr=1e-3,\n    warmup_steps=4000,\n    save_path = '/kaggle/working'\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    # convert config to torch tensors ONCE\n    mean_vec = torch.tensor(Config.MEAN_VECTOR, dtype=torch.float32).to(device)\n    std_vec  = torch.tensor(Config.STD_VECTOR, dtype=torch.float32).to(device)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.98), eps=1e-9)\n\n    def lr_schedule(step):\n        a1 = (step + 1) ** (-0.5)\n        a2 = (step + 1) * (warmup_steps ** -1.5)\n        return (model.d_model ** -0.5) * min(a1,a2)\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_schedule)\n\n    criterion = PhysicsWeightedMSELoss(w_pos=1.0,w_vel=0.2,w_acc=0.1,w_dir=0.05)\n    step_count = 0\n    best_val_loss = float('inf')\n    train_losses, val_losses = [], []\n    for epoch in range(1, epochs+1):\n        # ---------------------------------------------------\n        #  TRAIN\n        # ---------------------------------------------------\n        model.train()\n        train_loss = 0.0\n        train_evaluation_metric = 0.0\n        for x_seq, t_norm, y_true in train_loader:\n            x_seq = x_seq.to(device)\n            t_norm = t_norm.to(device)\n            y_true = y_true.to(device)\n\n            optimizer.zero_grad()\n            y_pred = model(x_seq, t_norm)\n            y_pred = torch.cat([y_pred[\"pos\"], y_pred[\"vel\"], y_pred[\"acc\"], y_pred[\"dir\"]], dim=1) \n            real_pred = y_pred * std_vec + mean_vec\n            real_true = y_true * std_vec + mean_vec\n            loss, train_stats = criterion(real_pred, real_true)\n            loss.backward()\n            # torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n            optimizer.step()\n            scheduler.step()\n\n            train_loss += loss.item()\n            train_evaluation_metric += train_stats['loss_pos']\n            step_count += 1\n            \n        train_loss = train_loss/len(train_loader)\n        train_evaluation_metric = train_evaluation_metric/len(train_loader)\n        # ---------------------------------------------------\n        #  VALIDATION\n        # ---------------------------------------------------\n        model.eval()\n        val_loss_total = 0.0\n        val_evaluation_metric = 0.0\n        with torch.no_grad():\n            for x_seq, t_norm, y_true in val_loader:\n                x_seq = x_seq.to(device)\n                t_norm = t_norm.to(device)\n                y_true = y_true.to(device)\n\n                y_pred = model(x_seq, t_norm)\n                y_pred = torch.cat([y_pred[\"pos\"], y_pred[\"vel\"], y_pred[\"acc\"], y_pred[\"dir\"]], dim=1)\n                real_pred = y_pred * std_vec + mean_vec\n                real_true = y_true * std_vec + mean_vec\n                loss, val_stats = criterion(real_pred, real_true)\n                val_loss_total += loss.item()\n                val_evaluation_metric += val_stats['loss_pos']\n        \n        val_loss = val_loss_total / len(val_loader)\n        val_evaluation_metric = val_evaluation_metric / len(val_loader)\n\n        # ---------------------------------------------------\n        # LOG  RESULTS\n        # ---------------------------------------------------\n        print(\n            f\"Epoch {epoch:03d} | \"\n            f\"Train Loss: {train_loss:.6f} | \"\n            f\"Train eval_metric: {train_evaluation_metric:.6f} | \"\n            f\"Val Loss: {val_loss:.6f} | \"\n            f\"Val eval_metric: {val_evaluation_metric:.6f} | \"\n        )\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n\n        # ---------------------------------------------------\n        # SAVE BEST MODEL\n        # ---------------------------------------------------\n        if save_path is not None and val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), os.path.join(save_path, 'best_model_v3.pth'))\n    return train_losses, val_losses","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:36.571405Z","iopub.execute_input":"2025-12-01T21:25:36.571591Z","iopub.status.idle":"2025-12-01T21:25:36.587853Z","shell.execute_reply.started":"2025-12-01T21:25:36.571570Z","shell.execute_reply":"2025-12-01T21:25:36.587305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(model, features, test_input, test_template, seq_len=8):\n\n    device = next(model.parameters()).device\n\n    # ENGINEER FEATURES\n    test_features = engineer_features(test_input)\n    test_features = add_sequence_features_catboost(test_features)\n\n    g_in = test_features.sort_values(['game_id','play_id','nfl_id','frame_id']) \\\n                        .groupby(['game_id','play_id','nfl_id'])\n\n    num_out_map = (\n        test_template.groupby(['game_id','play_id','nfl_id'])['frame_id']\n        .max()\n        .to_dict()\n    )\n\n    pred_x_list, pred_y_list, id_list = [], [], []\n\n    for _, row in test_template.iterrows():\n\n        gid, pid, nid, fid = row['game_id'], row['play_id'], row['nfl_id'], row['frame_id']\n        \n        num_out = num_out_map[(gid, pid, nid)] \n\n        t_norm = fid / num_out\n\n        key = (gid, pid, nid)\n        if key not in g_in.groups:\n            pred_x_list.append(0.0)\n            pred_y_list.append(0.0)\n            id_list.append(f\"{gid}_{pid}_{nid}_{fid}\")\n            continue\n\n        inp = g_in.get_group(key)\n        feat_mat = inp[features].values\n\n        # BUILD SEQUENCE\n        if feat_mat.shape[0] >= seq_len:\n            base_seq = feat_mat[-seq_len:, :]\n        else:\n            pad = np.repeat(feat_mat[0:1, :], seq_len - feat_mat.shape[0], axis=0)\n            base_seq = np.concatenate([pad, feat_mat], axis=0)\n\n        x_seq_tensor = torch.tensor(base_seq, dtype=torch.float32).unsqueeze(0).to(device)\n        t_tensor = torch.tensor([t_norm], dtype=torch.float32).to(device)\n\n        with torch.no_grad():\n            out = model(x_seq_tensor, t_tensor).squeeze(0).cpu().numpy()\n\n        pred_x_list.append(out[0])\n        pred_y_list.append(out[1])\n        id_list.append(f\"{gid}_{pid}_{nid}_{fid}\")\n\n    return pd.DataFrame({'x': pred_x_list, 'y': pred_y_list})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:36.588491Z","iopub.execute_input":"2025-12-01T21:25:36.588683Z","iopub.status.idle":"2025-12-01T21:25:36.606663Z","shell.execute_reply.started":"2025-12-01T21:25:36.588667Z","shell.execute_reply":"2025-12-01T21:25:36.606090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_pipeline(input_df, output_df):\n    \n    input_df = engineer_features(input_df)\n    # output_df = engineer_features(output_df)\n    # input_df = add_sequence_features_catboost(input_df)\n    train_df = create_training_dataset(input_df, output_df)\n    \n    seq_feature_cols = initiate_feature_cols()\n    \n    available_features = [col for col in seq_feature_cols if col in train_df.columns]\n    print(f\"Available features: {len(available_features)}\")\n    \n    train_df = train_df.dropna(subset=available_features + ['target_x', 'target_y'])\n    \n    seq_feature_cols = [c for c in seq_feature_cols if c in input_df.columns]\n    \n    sequences, times, targets = build_attention_sequences(input_df, output_df, seq_feature_cols, seq_len=Config.SEQ_LEN)\n    print(\"Sequences shape:\", sequences.shape)\n    print(\"Times shape:\", times.shape)\n    print(\"Targets shape:\", targets.shape)\n\n    # Train/val split\n    idx_train, idx_val = train_test_split(np.arange(len(sequences)), test_size=0.1, random_state=42)\n    \n    train_dataset = AttentionNFLDataset(sequences[idx_train], times[idx_train], targets[idx_train])\n    val_dataset   = AttentionNFLDataset(sequences[idx_val],   times[idx_val],   targets[idx_val])\n    \n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n    n_features = len(seq_feature_cols)\n    model = AttentionNFL(n_features=n_features, d_model=128, n_heads=16, num_layers=3,seq_len=Config.SEQ_LEN)\n    \n    train_losses, val_losses = train_attention_model(model,train_loader,val_loader,epochs=500,base_lr=1e-3)\n    torch.save(model.state_dict(), '/kaggle/working/Transformer_v7.pth')\n    # results = inference(model, available_features, test_input, test_template, Config.SEQ_LEN)\n    return model, train_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:36.607245Z","iopub.execute_input":"2025-12-01T21:25:36.607429Z","iopub.status.idle":"2025-12-01T21:25:36.623525Z","shell.execute_reply.started":"2025-12-01T21:25:36.607415Z","shell.execute_reply":"2025-12-01T21:25:36.623000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loading input files...\")\ninput_files, input_df = read_files(os.path.join(Config.BASE_DIR, 'train/input_*.csv'))\nprint(f\"Loaded {len(input_files)} input files.\")\n\nprint(\"\\nLoading output files...\")\noutput_files, output_df = read_files(os.path.join(Config.BASE_DIR, 'train/output_*.csv'))\nprint(f\"Loaded {len(output_files)} output files.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T21:25:36.625169Z","iopub.execute_input":"2025-12-01T21:25:36.625480Z","execution_failed":"2025-12-01T22:45:31.862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, train_losses, val_losses = train_pipeline(input_df, output_df)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-01T22:45:31.873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(train_losses, label='Train loss')\nplt.plot(val_losses, label='Validation loss')\nplt.xlabel('epochs')\nplt.ylabel('MSE Loss')\nplt.legend()\nplt.savefig('/kaggle/working/loss_curve.png',dpi=300)\nplt.show()","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"execution_failed":"2025-12-01T22:45:31.874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}