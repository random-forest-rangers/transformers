{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"},{"sourceId":13906109,"sourceType":"datasetVersion","datasetId":8860034}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4ddbfe38","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pickle\nimport joblib\nfrom pathlib import Path\nimport warnings\nimport os\nimport glob\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n# from catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error, mean_squared_log_error, mean_absolute_percentage_error, mean_tweedie_deviance\nfrom sklearn.model_selection import train_test_split\nimport kaggle_evaluation.nfl_inference_server\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:41:14.989608Z","iopub.execute_input":"2025-11-28T09:41:14.990243Z","iopub.status.idle":"2025-11-28T09:41:14.995577Z","shell.execute_reply.started":"2025-11-28T09:41:14.990219Z","shell.execute_reply":"2025-11-28T09:41:14.994839Z"}},"outputs":[],"execution_count":7},{"id":"3677c925","cell_type":"code","source":"class Config:\n    BASE_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n    CATBOOST_MODEL_PATH = \"Fall2025/AML/Project/models/catboost_5fold_models.pkl\"\n    LSTM_MODEL_DIR = \"Fall2025/AML/Project/models/output\"\n    \n    ENSEMBLE_WEIGHTS = { \n        'catboost': 0.5,\n        'lstm': 0.5\n    }\n    \n    ROLE_SPECIFIC_WEIGHTS = {\n        'Passer': {'catboost': 0.6, 'lstm': 0.4},\n        'Targeted Receiver': {'catboost': 0.4, 'lstm': 0.6},\n        'Defensive Coverage': {'catboost': 0.45, 'lstm': 0.55},\n        'default': {'catboost': 0.5, 'lstm': 0.5}\n    }\n    \n    USE_ROLE_SPECIFIC_WEIGHTS = False\n    \n    LSTM_N_FOLDS = 5\n    LSTM_WINDOW_SIZE = 8\n    SEQ_LEN = 8\n    \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:41:15.076172Z","iopub.execute_input":"2025-11-28T09:41:15.076495Z","iopub.status.idle":"2025-11-28T09:41:15.081744Z","shell.execute_reply.started":"2025-11-28T09:41:15.076471Z","shell.execute_reply":"2025-11-28T09:41:15.081068Z"}},"outputs":[],"execution_count":8},{"id":"a6f3af7f","cell_type":"code","source":"def height_to_feet(height_str):\n    try:\n        ft, inches = map(int, height_str.split('-'))\n        return ft + inches/12\n    except:\n        return None\n        \ndef engineer_features(df):\n    \"\"\"Create physics-based features for CatBoost models\"\"\"\n    df = df.copy()\n    \n    df['velocity_x'] = df['s'] * np.cos(np.radians(df['dir']))\n    df['velocity_y'] = df['s'] * np.sin(np.radians(df['dir']))\n    \n    df['dist_to_ball'] = np.sqrt(\n        (df['x'] - df['ball_land_x'])**2 + \n        (df['y'] - df['ball_land_y'])**2\n    )\n    \n    df['angle_to_ball'] = np.arctan2(\n        df['ball_land_y'] - df['y'],\n        df['ball_land_x'] - df['x']\n    )\n    \n    df['velocity_toward_ball'] = (\n        df['velocity_x'] * np.cos(df['angle_to_ball']) + \n        df['velocity_y'] * np.sin(df['angle_to_ball'])\n    )\n    \n    df['time_to_ball'] = df['num_frames_output'] / 10.0\n    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n    \n    df['role_targeted_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n    df['role_defensive_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n    df['role_passer'] = (df['player_role'] == 'Passer').astype(int)\n    df['side_offense'] = (df['player_side'] == 'Offense').astype(int)\n    \n    height_parts = df['player_height'].str.split('-', expand=True)\n    df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703\n    \n    df['acceleration_x'] = df['a'] * np.cos(np.radians(df['dir']))\n    df['acceleration_y'] = df['a'] * np.sin(np.radians(df['dir']))\n    df['distance_to_target_x'] = df['ball_land_x'] - df['x']\n    df['distance_to_target_y'] = df['ball_land_y'] - df['y']\n    df['speed_squared'] = df['s'] ** 2\n    df['accel_magnitude'] = np.sqrt(df['acceleration_x']**2 + df['acceleration_y']**2)\n    df['velocity_alignment'] = np.cos(df['angle_to_ball'] - np.radians(df['dir']))\n    \n    df['expected_x_at_ball'] = df['x'] + df['velocity_x'] * df['time_to_ball']\n    df['expected_y_at_ball'] = df['y'] + df['velocity_y'] * df['time_to_ball']\n    df['error_from_ball_x'] = df['expected_x_at_ball'] - df['ball_land_x']\n    df['error_from_ball_y'] = df['expected_y_at_ball'] - df['ball_land_y']\n    df['error_from_ball'] = np.sqrt(df['error_from_ball_x']**2 + df['error_from_ball_y']**2)\n    \n    df['momentum_x'] = df['player_weight'] * df['velocity_x']\n    df['momentum_y'] = df['player_weight'] * df['velocity_y']\n    df['kinetic_energy'] = 0.5 * df['player_weight'] * df['speed_squared']\n    \n    df['angle_diff'] = np.abs(df['o'] - np.degrees(df['angle_to_ball']))\n    df['angle_diff'] = np.minimum(df['angle_diff'], 360 - df['angle_diff'])\n    \n    df['time_squared'] = df['time_to_ball'] ** 2\n    df['dist_squared'] = df['dist_to_ball'] ** 2\n    df['weighted_dist_by_time'] = df['dist_to_ball'] / (df['time_to_ball'] + 0.1)\n    \n    return df\n\ndef add_sequence_features_catboost(df):\n    \"\"\"Add temporal features using lag and rolling statistics\"\"\"\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    group_cols = ['game_id', 'play_id', 'nfl_id']\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            if col in df.columns:\n                df[f'{col}_rolling_mean_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n                df[f'{col}_rolling_std_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n    \n    for col in ['velocity_x', 'velocity_y']:\n        if col in df.columns:\n            df[f'{col}_delta'] = df.groupby(group_cols)[col].diff()\n    \n    return df\n\ndef create_training_dataset(input_df, output_df):\n    output_df = output_df.copy()\n    output_df['id'] = (output_df['game_id'].astype(str) + '_' + \n                    output_df['play_id'].astype(str) + '_' + \n                    output_df['nfl_id'].astype(str) + '_' + \n                    output_df['frame_id'].astype(str))\n    \n    output_df = output_df.rename(columns={'x': 'target_x', 'y': 'target_y'})\n    \n    input_agg = input_df.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n    \n    if 'frame_id' in input_agg.columns:\n        input_agg = input_agg.drop('frame_id', axis=1)\n    \n    merged = output_df.merge(\n        input_agg,\n        on=['game_id', 'play_id', 'nfl_id'],\n        how='left',\n        suffixes=('', '_input')\n    )\n    \n    return merged\n\ndef build_attention_sequences(input_df, output_df, seq_feature_cols, seq_len=8):\n    # Sort input by time\n    input_sorted = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    output_sorted = output_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    g_in = input_sorted.groupby(['game_id', 'play_id', 'nfl_id'])\n    g_out = output_sorted.groupby(['game_id', 'play_id', 'nfl_id'])\n    \n    common_keys = sorted(set(g_in.groups.keys()) & set(g_out.groups.keys()))\n    \n    sequences = []\n    times = []\n    targets = []\n    \n    for key in common_keys:\n        inp = g_in.get_group(key)\n        out = g_out.get_group(key)\n        \n        # base sequence of pre-throw frames\n        feat_mat = inp[seq_feature_cols].values  # (T_in, F)\n        if feat_mat.shape[0] >= seq_len:\n            base_seq = feat_mat[-seq_len:, :]\n        else:\n            # pad by repeating the first frame\n            pad = np.repeat(feat_mat[0:1, :], seq_len - feat_mat.shape[0], axis=0)\n            base_seq = np.concatenate([pad, feat_mat], axis=0)\n        \n        # same num_frames_output for this player\n        num_frames_output = inp['num_frames_output'].iloc[-1]\n        if num_frames_output <= 0:\n            continue\n        \n        for _, row in out.iterrows():\n            frame_id = row['frame_id']\n            t_norm = frame_id / num_frames_output  # normalized time in [0,1]\n            sequences.append(base_seq)\n            times.append(t_norm)\n            targets.append(row[['x', 'y']].values.astype(np.float32))\n    \n    sequences = np.stack(sequences)        # (N, seq_len, F)\n    times = np.array(times, dtype=np.float32)   # (N,)\n    targets = np.stack(targets)            # (N, 2)\n    \n    print(f\"Built {len(sequences)} sequence examples for attention model.\")\n    return sequences, times, targets\n\ndef read_files(file_path):\n    files = glob.glob(file_path)\n    df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n    return files, df\n\ndef initiate_feature_cols():\n    seq_feature_cols = [\n        'x', 'y', 's', 'a', 'o', 'dir',\n        'velocity_x', 'velocity_y', 'dist_to_ball', 'angle_to_ball',\n        'velocity_toward_ball', 'time_to_ball', 'orientation_diff',\n        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer',\n        'side_offense', 'height_inches', 'player_weight', 'bmi',\n        'ball_land_x', 'ball_land_y',\n        'acceleration_x', 'acceleration_y',\n        'distance_to_target_x', 'distance_to_target_y',\n        'speed_squared', 'accel_magnitude', 'velocity_alignment',\n        'expected_x_at_ball', 'expected_y_at_ball',\n        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        'angle_diff', 'time_squared', 'dist_squared', 'weighted_dist_by_time'\n    ]\n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            seq_feature_cols.append(f'{col}_lag{lag}')\n        \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            seq_feature_cols.append(f'{col}_rolling_mean_{window}')\n            seq_feature_cols.append(f'{col}_rolling_std_{window}')\n    \n    seq_feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n    return seq_feature_cols","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:41:15.283444Z","iopub.execute_input":"2025-11-28T09:41:15.284024Z","iopub.status.idle":"2025-11-28T09:41:15.306429Z","shell.execute_reply.started":"2025-11-28T09:41:15.284002Z","shell.execute_reply":"2025-11-28T09:41:15.305465Z"}},"outputs":[],"execution_count":9},{"id":"e160cea5","cell_type":"code","source":"class AttentionNFLDataset(Dataset):\n    def __init__(self, sequences, times, targets):\n        self.sequences = sequences\n        self.times = times\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        x_seq = torch.tensor(self.sequences[idx], dtype=torch.float32)  # (T, F)\n        t = torch.tensor(self.times[idx], dtype=torch.float32)          # scalar\n        y = torch.tensor(self.targets[idx], dtype=torch.float32)        # (2,)\n        return x_seq, t, y","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:41:15.307574Z","iopub.execute_input":"2025-11-28T09:41:15.308074Z","iopub.status.idle":"2025-11-28T09:41:15.326797Z","shell.execute_reply.started":"2025-11-28T09:41:15.308055Z","shell.execute_reply":"2025-11-28T09:41:15.326158Z"}},"outputs":[],"execution_count":10},{"id":"b6d4e2c8","cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=100):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n\n    def forward(self, x):\n        # x: (B, T, d_model)\n        T = x.size(1)\n        return x + self.pe[:, :T, :]\n\n\nclass AttentionNFL(nn.Module):\n    def __init__(self, n_features, d_model=128, n_heads=4, num_layers=3):\n        super().__init__()\n        self.d_model = d_model\n        \n        # project raw features up to model dimension\n        self.input_projection = nn.Linear(n_features, d_model)\n        self.pos_encoding = PositionalEncoding(d_model)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=4*d_model,\n            dropout=0.1,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # time conditioning: t_norm ∈ [0,1] → embedding of size d_model\n        self.time_mlp = nn.Sequential(\n            nn.Linear(1, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, d_model),\n            nn.ReLU()\n        )\n        \n        # final head: [context; time_embed] → x,y\n        self.head = nn.Sequential(\n            nn.Linear(2*d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, 2)\n        )\n\n    def forward(self, x_seq, t_norm):\n        \"\"\"\n        x_seq: (B, T, n_features)\n        t_norm: (B,)  normalized frame_id / num_frames_output\n        returns: (B, 2)  predicted (x, y)\n        \"\"\"\n        # encode sequence\n        x = self.input_projection(x_seq)          # (B, T, d_model)\n        x = self.pos_encoding(x)                  # (B, T, d_model)\n        enc = self.transformer(x)                 # (B, T, d_model)\n        \n        # use final timestep as context\n        context = enc[:, -1, :]                   # (B, d_model)\n        \n        # embed time\n        t_norm = t_norm.view(-1, 1)               # (B, 1)\n        t_embed = self.time_mlp(t_norm)           # (B, d_model)\n        \n        # combine and predict\n        h = torch.cat([context, t_embed], dim=-1) # (B, 2*d_model)\n        out = self.head(h)                        # (B, 2)\n        return out","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:41:15.337962Z","iopub.execute_input":"2025-11-28T09:41:15.338484Z","iopub.status.idle":"2025-11-28T09:41:15.347794Z","shell.execute_reply.started":"2025-11-28T09:41:15.338458Z","shell.execute_reply":"2025-11-28T09:41:15.347013Z"}},"outputs":[],"execution_count":11},{"id":"d70e0748","cell_type":"code","source":"def train_attention_model(\n    model,\n    train_loader,\n    val_loader,\n    epochs=10,\n    base_lr=1e-3,\n    warmup_steps=4000,\n    save_path = '/kaggle/working'\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.98), eps=1e-9)\n\n    def lr_schedule(step):\n        return min(\n            (step + 1) ** (-0.5),\n            (step + 1) * (warmup_steps ** -1.5)\n        )\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_schedule)\n\n    criterion = nn.MSELoss()\n    step_count = 0\n    best_val_loss = float('inf')\n    train_losses, val_losses = [], []\n    for epoch in range(1, epochs+1):\n        # ---------------------------------------------------\n        #  TRAIN\n        # ---------------------------------------------------\n        model.train()\n        train_loss = 0.0\n        for x_seq, t_norm, y in train_loader:\n            x_seq = x_seq.to(device)\n            t_norm = t_norm.to(device)\n            y = y.to(device)\n\n            optimizer.zero_grad()\n            y_pred = model(x_seq, t_norm)\n            loss = criterion(y_pred, y)\n            loss.backward()\n\n            optimizer.step()\n            scheduler.step()\n\n            train_loss += loss.item()\n            step_count += 1\n            \n        train_loss = train_loss/len(train_loader)\n        # ---------------------------------------------------\n        #  VALIDATION\n        # ---------------------------------------------------\n        model.eval()\n        val_loss_total = 0.0\n\n        with torch.no_grad():\n            for x_seq, t_norm, y in val_loader:\n                x_seq = x_seq.to(device)\n                t_norm = t_norm.to(device)\n                y = y.to(device)\n\n                y_pred = model(x_seq, t_norm)\n                loss = criterion(y_pred, y)\n\n                val_loss_total += loss.item()\n\n        val_loss = val_loss_total / len(val_loader)\n\n\n\n        # ---------------------------------------------------\n        # LOG  RESULTS\n        # ---------------------------------------------------\n        print(\n            f\"Epoch {epoch:03d} | \"\n            f\"Train Loss: {train_loss:.6f} | \"\n            f\"Val Loss: {val_loss:.6f}\"\n        )\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n\n        # ---------------------------------------------------\n        # SAVE BEST MODEL\n        # ---------------------------------------------------\n        if save_path is not None and val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), os.path.join(save_path, 'best_model.pth'))\n            print(f\"  → Saved new best model to {os.path.join(save_path, 'best_model.pth')} (Val Loss {val_loss:.6f})\")\n\n    return train_losses, val_losses","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:26:33.424003Z","iopub.execute_input":"2025-11-28T10:26:33.424766Z","iopub.status.idle":"2025-11-28T10:26:33.434840Z","shell.execute_reply.started":"2025-11-28T10:26:33.424738Z","shell.execute_reply":"2025-11-28T10:26:33.433894Z"}},"outputs":[],"execution_count":28},{"id":"7408bcb2-7fc0-4719-815b-5b909f4bb1ae","cell_type":"code","source":"def inference(model, features, test_input, test_template, seq_len=8):\n\n    device = next(model.parameters()).device\n\n    # ENGINEER FEATURES\n    test_features = engineer_features(test_input)\n    test_features = add_sequence_features_catboost(test_features)\n\n    # GROUP PRE-THROW FRAMES\n    g_in = test_features.sort_values(['game_id','play_id','nfl_id','frame_id']) \\\n                        .groupby(['game_id','play_id','nfl_id'])\n\n    # COMPUTE num_frames_output FOR MOCK TEST\n    num_out_map = (\n        test_template.groupby(['game_id','play_id','nfl_id'])['frame_id']\n        .max()\n        .to_dict()\n    )\n\n    pred_x_list, pred_y_list, id_list = [], [], []\n\n    for _, row in test_template.iterrows():\n\n        gid, pid, nid, fid = row['game_id'], row['play_id'], row['nfl_id'], row['frame_id']\n        \n        num_out = num_out_map[(gid, pid, nid)]   # ★ REPLACEMENT ★\n\n        t_norm = fid / num_out\n\n        key = (gid, pid, nid)\n        if key not in g_in.groups:\n            pred_x_list.append(0.0)\n            pred_y_list.append(0.0)\n            id_list.append(f\"{gid}_{pid}_{nid}_{fid}\")\n            continue\n\n        inp = g_in.get_group(key)\n        feat_mat = inp[features].values\n\n        # BUILD SEQUENCE\n        if feat_mat.shape[0] >= seq_len:\n            base_seq = feat_mat[-seq_len:, :]\n        else:\n            pad = np.repeat(feat_mat[0:1, :], seq_len - feat_mat.shape[0], axis=0)\n            base_seq = np.concatenate([pad, feat_mat], axis=0)\n\n        x_seq_tensor = torch.tensor(base_seq, dtype=torch.float32).unsqueeze(0).to(device)\n        t_tensor = torch.tensor([t_norm], dtype=torch.float32).to(device)\n\n        with torch.no_grad():\n            out = model(x_seq_tensor, t_tensor).squeeze(0).cpu().numpy()\n\n        pred_x_list.append(out[0])\n        pred_y_list.append(out[1])\n        id_list.append(f\"{gid}_{pid}_{nid}_{fid}\")\n\n    return pd.DataFrame({'x': pred_x_list, 'y': pred_y_list})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:26:36.662237Z","iopub.execute_input":"2025-11-28T10:26:36.663029Z","iopub.status.idle":"2025-11-28T10:26:36.671324Z","shell.execute_reply.started":"2025-11-28T10:26:36.663004Z","shell.execute_reply":"2025-11-28T10:26:36.670681Z"}},"outputs":[],"execution_count":29},{"id":"c1009ba4-55d8-44ec-8471-1460ca5d5359","cell_type":"code","source":"def train_pipeline(input_df, output_df):\n    \n    input_df = engineer_features(input_df)\n    # input_df = add_sequence_features_catboost(input_df)\n    train_df = create_training_dataset(input_df, output_df)\n    \n    seq_feature_cols = initiate_feature_cols()\n    \n    available_features = [col for col in seq_feature_cols if col in train_df.columns]\n    print(f\"Available features: {len(available_features)}\")\n    \n    train_df = train_df.dropna(subset=available_features + ['target_x', 'target_y'])\n    \n    seq_feature_cols = [c for c in seq_feature_cols if c in input_df.columns]\n    \n    sequences, times, targets = build_attention_sequences(input_df, output_df, seq_feature_cols, seq_len=Config.SEQ_LEN)\n    print(\"Sequences shape:\", sequences.shape)\n    print(\"Times shape:\", times.shape)\n    print(\"Targets shape:\", targets.shape)\n\n    # Train/val split at sample level\n    idx_train, idx_val = train_test_split(np.arange(len(sequences)), test_size=0.1, random_state=42)\n    \n    train_dataset = AttentionNFLDataset(sequences[idx_train], times[idx_train], targets[idx_train])\n    val_dataset   = AttentionNFLDataset(sequences[idx_val],   times[idx_val],   targets[idx_val])\n    \n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n    n_features = len(seq_feature_cols)\n    model = AttentionNFL(n_features=n_features, d_model=128, n_heads=8, num_layers=3)\n    \n    train_losses, val_losses = train_attention_model(model,train_loader,val_loader,epochs=250,base_lr=1e-3)\n    torch.save(model.state_dict(), '/kaggle/working/Transformer_v1.pth')\n    # results = inference(model, available_features, test_input, test_template, Config.SEQ_LEN)\n    return model, train_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:26:37.334600Z","iopub.execute_input":"2025-11-28T10:26:37.335551Z","iopub.status.idle":"2025-11-28T10:26:37.342753Z","shell.execute_reply.started":"2025-11-28T10:26:37.335526Z","shell.execute_reply":"2025-11-28T10:26:37.341950Z"}},"outputs":[],"execution_count":30},{"id":"23be6659-5e34-4056-a74d-3c1f8bab15db","cell_type":"code","source":"print(\"Loading input files...\")\ninput_files, input_df = read_files(os.path.join(Config.BASE_DIR, 'train/input_*.csv'))\nprint(f\"Loaded {len(input_files)} input files.\")\n\nprint(\"\\nLoading output files...\")\noutput_files, output_df = read_files(os.path.join(Config.BASE_DIR, 'train/output_*.csv'))\nprint(f\"Loaded {len(output_files)} output files.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:58:58.471000Z","iopub.execute_input":"2025-11-28T09:58:58.471713Z","iopub.status.idle":"2025-11-28T09:59:09.904094Z","shell.execute_reply.started":"2025-11-28T09:58:58.471682Z","shell.execute_reply":"2025-11-28T09:59:09.903385Z"}},"outputs":[{"name":"stdout","text":"Loading input files...\nLoaded 18 input files.\n\nLoading output files...\nLoaded 18 output files.\n","output_type":"stream"}],"execution_count":23},{"id":"fc23b934-6f15-4383-b665-dd74458bf416","cell_type":"code","source":"model, train_losses, val_losses = train_pipeline(input_df, output_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:26:42.309728Z","iopub.execute_input":"2025-11-28T10:26:42.310018Z","iopub.status.idle":"2025-11-28T11:33:25.391651Z","shell.execute_reply.started":"2025-11-28T10:26:42.309997Z","shell.execute_reply":"2025-11-28T11:33:25.390678Z"}},"outputs":[{"name":"stdout","text":"Available features: 41\nBuilt 562936 sequence examples for attention model.\nSequences shape: (562936, 8, 41)\nTimes shape: (562936,)\nTargets shape: (562936, 2)\nEpoch 001 | Train Loss: 2542.839079 | Val Loss: 2462.568501\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2462.568501)\nEpoch 002 | Train Loss: 2360.188831 | Val Loss: 2240.366590\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2240.366590)\nEpoch 003 | Train Loss: 2010.910231 | Val Loss: 1711.351220\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1711.351220)\nEpoch 004 | Train Loss: 1269.425723 | Val Loss: 800.542413\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 800.542413)\nEpoch 005 | Train Loss: 471.978819 | Val Loss: 227.047825\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 227.047825)\nEpoch 006 | Train Loss: 147.879627 | Val Loss: 85.527197\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 85.527197)\nEpoch 007 | Train Loss: 69.552760 | Val Loss: 37.196210\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 37.196210)\nEpoch 008 | Train Loss: 35.378594 | Val Loss: 20.099411\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 20.099411)\nEpoch 009 | Train Loss: 24.720382 | Val Loss: 15.357070\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 15.357070)\nEpoch 010 | Train Loss: 20.168329 | Val Loss: 12.902242\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 12.902242)\nEpoch 011 | Train Loss: 17.485288 | Val Loss: 11.800033\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 11.800033)\nEpoch 012 | Train Loss: 15.797051 | Val Loss: 10.981764\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 10.981764)\nEpoch 013 | Train Loss: 14.576549 | Val Loss: 9.972386\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 9.972386)\nEpoch 014 | Train Loss: 13.582357 | Val Loss: 9.896126\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 9.896126)\nEpoch 015 | Train Loss: 12.823153 | Val Loss: 9.926009\nEpoch 016 | Train Loss: 12.189719 | Val Loss: 9.235663\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 9.235663)\nEpoch 017 | Train Loss: 11.682061 | Val Loss: 8.449415\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 8.449415)\nEpoch 018 | Train Loss: 11.233728 | Val Loss: 8.686098\nEpoch 019 | Train Loss: 10.905773 | Val Loss: 8.158012\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 8.158012)\nEpoch 020 | Train Loss: 10.515887 | Val Loss: 8.275775\nEpoch 021 | Train Loss: 10.238838 | Val Loss: 7.709233\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 7.709233)\nEpoch 022 | Train Loss: 9.949409 | Val Loss: 7.438256\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 7.438256)\nEpoch 023 | Train Loss: 9.717541 | Val Loss: 7.606823\nEpoch 024 | Train Loss: 9.493818 | Val Loss: 7.513256\nEpoch 025 | Train Loss: 9.288847 | Val Loss: 6.934803\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 6.934803)\nEpoch 026 | Train Loss: 9.108305 | Val Loss: 7.093960\nEpoch 027 | Train Loss: 8.955287 | Val Loss: 6.824286\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 6.824286)\nEpoch 028 | Train Loss: 8.784934 | Val Loss: 6.753270\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 6.753270)\nEpoch 029 | Train Loss: 8.631408 | Val Loss: 6.485281\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 6.485281)\nEpoch 030 | Train Loss: 8.505192 | Val Loss: 6.466649\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 6.466649)\nEpoch 031 | Train Loss: 8.345751 | Val Loss: 6.494111\nEpoch 032 | Train Loss: 8.238443 | Val Loss: 6.536719\nEpoch 033 | Train Loss: 8.091863 | Val Loss: 6.908790\nEpoch 034 | Train Loss: 7.974304 | Val Loss: 6.107781\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 6.107781)\nEpoch 035 | Train Loss: 7.842589 | Val Loss: 6.066885\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 6.066885)\nEpoch 036 | Train Loss: 7.726619 | Val Loss: 6.107934\nEpoch 037 | Train Loss: 7.611998 | Val Loss: 5.998178\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 5.998178)\nEpoch 038 | Train Loss: 7.500493 | Val Loss: 5.959413\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 5.959413)\nEpoch 039 | Train Loss: 7.412666 | Val Loss: 5.889613\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 5.889613)\nEpoch 040 | Train Loss: 7.298155 | Val Loss: 6.001604\nEpoch 041 | Train Loss: 7.186577 | Val Loss: 5.539018\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 5.539018)\nEpoch 042 | Train Loss: 7.075925 | Val Loss: 5.506112\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 5.506112)\nEpoch 043 | Train Loss: 6.957723 | Val Loss: 5.524408\nEpoch 044 | Train Loss: 6.886619 | Val Loss: 5.282207\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 5.282207)\nEpoch 045 | Train Loss: 6.773118 | Val Loss: 5.258200\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 5.258200)\nEpoch 046 | Train Loss: 6.667070 | Val Loss: 5.170012\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 5.170012)\nEpoch 047 | Train Loss: 6.569745 | Val Loss: 5.120064\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 5.120064)\nEpoch 048 | Train Loss: 6.476546 | Val Loss: 4.909187\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 4.909187)\nEpoch 049 | Train Loss: 6.373745 | Val Loss: 5.117385\nEpoch 050 | Train Loss: 6.292504 | Val Loss: 5.047551\nEpoch 051 | Train Loss: 6.202442 | Val Loss: 4.692574\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 4.692574)\nEpoch 052 | Train Loss: 6.099220 | Val Loss: 4.638095\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 4.638095)\nEpoch 053 | Train Loss: 6.006923 | Val Loss: 4.687145\nEpoch 054 | Train Loss: 5.912926 | Val Loss: 4.608599\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 4.608599)\nEpoch 055 | Train Loss: 5.824216 | Val Loss: 4.443539\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 4.443539)\nEpoch 056 | Train Loss: 5.727088 | Val Loss: 4.270047\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 4.270047)\nEpoch 057 | Train Loss: 5.633646 | Val Loss: 4.377560\nEpoch 058 | Train Loss: 5.549614 | Val Loss: 4.123609\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 4.123609)\nEpoch 059 | Train Loss: 5.487590 | Val Loss: 4.233996\nEpoch 060 | Train Loss: 5.398533 | Val Loss: 4.141129\nEpoch 061 | Train Loss: 5.304971 | Val Loss: 4.000520\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 4.000520)\nEpoch 062 | Train Loss: 5.235248 | Val Loss: 4.091686\nEpoch 063 | Train Loss: 5.156637 | Val Loss: 3.915297\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.915297)\nEpoch 064 | Train Loss: 5.100053 | Val Loss: 3.761623\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.761623)\nEpoch 065 | Train Loss: 5.028867 | Val Loss: 3.759887\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.759887)\nEpoch 066 | Train Loss: 4.973430 | Val Loss: 3.792375\nEpoch 067 | Train Loss: 4.904129 | Val Loss: 3.644485\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.644485)\nEpoch 068 | Train Loss: 4.845735 | Val Loss: 3.575033\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.575033)\nEpoch 069 | Train Loss: 4.796409 | Val Loss: 3.584344\nEpoch 070 | Train Loss: 4.739047 | Val Loss: 3.610981\nEpoch 071 | Train Loss: 4.698163 | Val Loss: 3.446216\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.446216)\nEpoch 072 | Train Loss: 4.650694 | Val Loss: 3.696350\nEpoch 073 | Train Loss: 4.617066 | Val Loss: 3.522623\nEpoch 074 | Train Loss: 4.556687 | Val Loss: 3.451987\nEpoch 075 | Train Loss: 4.520994 | Val Loss: 3.338637\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.338637)\nEpoch 076 | Train Loss: 4.474608 | Val Loss: 3.589196\nEpoch 077 | Train Loss: 4.436403 | Val Loss: 3.272502\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.272502)\nEpoch 078 | Train Loss: 4.385870 | Val Loss: 3.293433\nEpoch 079 | Train Loss: 4.348112 | Val Loss: 3.182857\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.182857)\nEpoch 080 | Train Loss: 4.309177 | Val Loss: 3.095928\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.095928)\nEpoch 081 | Train Loss: 4.270304 | Val Loss: 3.153465\nEpoch 082 | Train Loss: 4.233020 | Val Loss: 3.259397\nEpoch 083 | Train Loss: 4.196059 | Val Loss: 3.058125\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.058125)\nEpoch 084 | Train Loss: 4.155584 | Val Loss: 3.063375\nEpoch 085 | Train Loss: 4.118523 | Val Loss: 3.030620\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 3.030620)\nEpoch 086 | Train Loss: 4.077055 | Val Loss: 2.889886\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.889886)\nEpoch 087 | Train Loss: 4.049776 | Val Loss: 2.915828\nEpoch 088 | Train Loss: 4.009624 | Val Loss: 2.879637\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.879637)\nEpoch 089 | Train Loss: 3.977343 | Val Loss: 2.947931\nEpoch 090 | Train Loss: 3.940051 | Val Loss: 2.840448\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.840448)\nEpoch 091 | Train Loss: 3.901701 | Val Loss: 2.954509\nEpoch 092 | Train Loss: 3.868000 | Val Loss: 2.795797\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.795797)\nEpoch 093 | Train Loss: 3.836974 | Val Loss: 2.877990\nEpoch 094 | Train Loss: 3.798795 | Val Loss: 2.835579\nEpoch 095 | Train Loss: 3.771003 | Val Loss: 2.674136\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.674136)\nEpoch 096 | Train Loss: 3.734944 | Val Loss: 2.676971\nEpoch 097 | Train Loss: 3.697028 | Val Loss: 2.646402\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.646402)\nEpoch 098 | Train Loss: 3.674033 | Val Loss: 2.636760\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.636760)\nEpoch 099 | Train Loss: 3.634548 | Val Loss: 2.611187\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.611187)\nEpoch 100 | Train Loss: 3.601132 | Val Loss: 2.496745\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.496745)\nEpoch 101 | Train Loss: 3.582193 | Val Loss: 2.679115\nEpoch 102 | Train Loss: 3.551224 | Val Loss: 2.522665\nEpoch 103 | Train Loss: 3.516661 | Val Loss: 2.548899\nEpoch 104 | Train Loss: 3.486883 | Val Loss: 2.429451\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.429451)\nEpoch 105 | Train Loss: 3.464377 | Val Loss: 2.463285\nEpoch 106 | Train Loss: 3.424385 | Val Loss: 2.456384\nEpoch 107 | Train Loss: 3.405435 | Val Loss: 2.402226\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.402226)\nEpoch 108 | Train Loss: 3.379862 | Val Loss: 2.365301\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.365301)\nEpoch 109 | Train Loss: 3.351913 | Val Loss: 2.433726\nEpoch 110 | Train Loss: 3.329864 | Val Loss: 2.426184\nEpoch 111 | Train Loss: 3.308711 | Val Loss: 2.279346\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.279346)\nEpoch 112 | Train Loss: 3.276333 | Val Loss: 2.210642\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.210642)\nEpoch 113 | Train Loss: 3.258770 | Val Loss: 2.295221\nEpoch 114 | Train Loss: 3.232298 | Val Loss: 2.246272\nEpoch 115 | Train Loss: 3.212388 | Val Loss: 2.202348\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.202348)\nEpoch 116 | Train Loss: 3.182474 | Val Loss: 2.195923\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.195923)\nEpoch 117 | Train Loss: 3.167618 | Val Loss: 2.232334\nEpoch 118 | Train Loss: 3.147188 | Val Loss: 2.159235\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.159235)\nEpoch 119 | Train Loss: 3.121240 | Val Loss: 2.121258\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.121258)\nEpoch 120 | Train Loss: 3.098534 | Val Loss: 2.179546\nEpoch 121 | Train Loss: 3.082914 | Val Loss: 2.232619\nEpoch 122 | Train Loss: 3.062188 | Val Loss: 2.094045\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.094045)\nEpoch 123 | Train Loss: 3.037271 | Val Loss: 2.098394\nEpoch 124 | Train Loss: 3.019496 | Val Loss: 2.059416\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.059416)\nEpoch 125 | Train Loss: 3.006559 | Val Loss: 2.026782\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 2.026782)\nEpoch 126 | Train Loss: 2.986101 | Val Loss: 2.152087\nEpoch 127 | Train Loss: 2.969945 | Val Loss: 1.960579\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.960579)\nEpoch 128 | Train Loss: 2.957285 | Val Loss: 1.971294\nEpoch 129 | Train Loss: 2.945286 | Val Loss: 1.973126\nEpoch 130 | Train Loss: 2.919829 | Val Loss: 1.959527\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.959527)\nEpoch 131 | Train Loss: 2.911201 | Val Loss: 1.954053\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.954053)\nEpoch 132 | Train Loss: 2.888588 | Val Loss: 1.961814\nEpoch 133 | Train Loss: 2.872048 | Val Loss: 1.996335\nEpoch 134 | Train Loss: 2.859489 | Val Loss: 1.895174\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.895174)\nEpoch 135 | Train Loss: 2.849015 | Val Loss: 1.949581\nEpoch 136 | Train Loss: 2.823384 | Val Loss: 1.884964\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.884964)\nEpoch 137 | Train Loss: 2.821332 | Val Loss: 1.832154\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.832154)\nEpoch 138 | Train Loss: 2.803884 | Val Loss: 1.832516\nEpoch 139 | Train Loss: 2.793031 | Val Loss: 1.829182\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.829182)\nEpoch 140 | Train Loss: 2.769955 | Val Loss: 1.835508\nEpoch 141 | Train Loss: 2.757288 | Val Loss: 1.896099\nEpoch 142 | Train Loss: 2.743756 | Val Loss: 1.830738\nEpoch 143 | Train Loss: 2.738179 | Val Loss: 1.828689\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.828689)\nEpoch 144 | Train Loss: 2.722491 | Val Loss: 1.824279\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.824279)\nEpoch 145 | Train Loss: 2.710424 | Val Loss: 1.761731\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.761731)\nEpoch 146 | Train Loss: 2.697830 | Val Loss: 1.799137\nEpoch 147 | Train Loss: 2.689594 | Val Loss: 1.776811\nEpoch 148 | Train Loss: 2.670482 | Val Loss: 1.823355\nEpoch 149 | Train Loss: 2.666834 | Val Loss: 1.697561\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.697561)\nEpoch 150 | Train Loss: 2.649202 | Val Loss: 1.844341\nEpoch 151 | Train Loss: 2.634917 | Val Loss: 1.724949\nEpoch 152 | Train Loss: 2.629615 | Val Loss: 1.808949\nEpoch 153 | Train Loss: 2.614538 | Val Loss: 1.780154\nEpoch 154 | Train Loss: 2.603894 | Val Loss: 1.743083\nEpoch 155 | Train Loss: 2.588672 | Val Loss: 1.794404\nEpoch 156 | Train Loss: 2.583761 | Val Loss: 1.672617\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.672617)\nEpoch 157 | Train Loss: 2.572033 | Val Loss: 1.800749\nEpoch 158 | Train Loss: 2.564563 | Val Loss: 1.689248\nEpoch 159 | Train Loss: 2.549435 | Val Loss: 1.669888\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.669888)\nEpoch 160 | Train Loss: 2.545829 | Val Loss: 1.714002\nEpoch 161 | Train Loss: 2.532803 | Val Loss: 1.725319\nEpoch 162 | Train Loss: 2.528780 | Val Loss: 1.669524\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.669524)\nEpoch 163 | Train Loss: 2.516898 | Val Loss: 1.661057\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.661057)\nEpoch 164 | Train Loss: 2.508876 | Val Loss: 1.613054\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.613054)\nEpoch 165 | Train Loss: 2.496853 | Val Loss: 1.607638\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.607638)\nEpoch 166 | Train Loss: 2.483735 | Val Loss: 1.715135\nEpoch 167 | Train Loss: 2.477994 | Val Loss: 1.600294\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.600294)\nEpoch 168 | Train Loss: 2.467685 | Val Loss: 1.610803\nEpoch 169 | Train Loss: 2.455448 | Val Loss: 1.607212\nEpoch 170 | Train Loss: 2.453415 | Val Loss: 1.662221\nEpoch 171 | Train Loss: 2.438288 | Val Loss: 1.631949\nEpoch 172 | Train Loss: 2.433163 | Val Loss: 1.568405\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.568405)\nEpoch 173 | Train Loss: 2.423868 | Val Loss: 1.586548\nEpoch 174 | Train Loss: 2.419564 | Val Loss: 1.657968\nEpoch 175 | Train Loss: 2.403245 | Val Loss: 1.534668\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.534668)\nEpoch 176 | Train Loss: 2.392063 | Val Loss: 1.586718\nEpoch 177 | Train Loss: 2.388357 | Val Loss: 1.637280\nEpoch 178 | Train Loss: 2.385176 | Val Loss: 1.545914\nEpoch 179 | Train Loss: 2.379671 | Val Loss: 1.574858\nEpoch 180 | Train Loss: 2.369251 | Val Loss: 1.558763\nEpoch 181 | Train Loss: 2.359256 | Val Loss: 1.555688\nEpoch 182 | Train Loss: 2.352784 | Val Loss: 1.513789\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.513789)\nEpoch 183 | Train Loss: 2.344136 | Val Loss: 1.609392\nEpoch 184 | Train Loss: 2.338621 | Val Loss: 1.455950\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.455950)\nEpoch 185 | Train Loss: 2.323516 | Val Loss: 1.473382\nEpoch 186 | Train Loss: 2.321252 | Val Loss: 1.576419\nEpoch 187 | Train Loss: 2.317463 | Val Loss: 1.472224\nEpoch 188 | Train Loss: 2.312329 | Val Loss: 1.492656\nEpoch 189 | Train Loss: 2.302536 | Val Loss: 1.527002\nEpoch 190 | Train Loss: 2.294289 | Val Loss: 1.457684\nEpoch 191 | Train Loss: 2.287763 | Val Loss: 1.480178\nEpoch 192 | Train Loss: 2.278353 | Val Loss: 1.445683\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.445683)\nEpoch 193 | Train Loss: 2.271067 | Val Loss: 1.445377\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.445377)\nEpoch 194 | Train Loss: 2.272476 | Val Loss: 1.414775\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.414775)\nEpoch 195 | Train Loss: 2.264368 | Val Loss: 1.470485\nEpoch 196 | Train Loss: 2.254165 | Val Loss: 1.424925\nEpoch 197 | Train Loss: 2.246436 | Val Loss: 1.414995\nEpoch 198 | Train Loss: 2.245151 | Val Loss: 1.408642\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.408642)\nEpoch 199 | Train Loss: 2.237118 | Val Loss: 1.389480\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.389480)\nEpoch 200 | Train Loss: 2.228359 | Val Loss: 1.454111\nEpoch 201 | Train Loss: 2.232434 | Val Loss: 1.471919\nEpoch 202 | Train Loss: 2.218700 | Val Loss: 1.403833\nEpoch 203 | Train Loss: 2.209142 | Val Loss: 1.444945\nEpoch 204 | Train Loss: 2.209180 | Val Loss: 1.440565\nEpoch 205 | Train Loss: 2.198468 | Val Loss: 1.429750\nEpoch 206 | Train Loss: 2.196526 | Val Loss: 1.426496\nEpoch 207 | Train Loss: 2.182613 | Val Loss: 1.446507\nEpoch 208 | Train Loss: 2.186719 | Val Loss: 1.422318\nEpoch 209 | Train Loss: 2.181430 | Val Loss: 1.449982\nEpoch 210 | Train Loss: 2.175178 | Val Loss: 1.391184\nEpoch 211 | Train Loss: 2.163163 | Val Loss: 1.426243\nEpoch 212 | Train Loss: 2.163980 | Val Loss: 1.384251\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.384251)\nEpoch 213 | Train Loss: 2.162076 | Val Loss: 1.401919\nEpoch 214 | Train Loss: 2.150902 | Val Loss: 1.386666\nEpoch 215 | Train Loss: 2.145459 | Val Loss: 1.365345\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.365345)\nEpoch 216 | Train Loss: 2.138897 | Val Loss: 1.378417\nEpoch 217 | Train Loss: 2.138067 | Val Loss: 1.390808\nEpoch 218 | Train Loss: 2.135941 | Val Loss: 1.427983\nEpoch 219 | Train Loss: 2.128191 | Val Loss: 1.394039\nEpoch 220 | Train Loss: 2.125534 | Val Loss: 1.403043\nEpoch 221 | Train Loss: 2.117078 | Val Loss: 1.364117\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.364117)\nEpoch 222 | Train Loss: 2.118206 | Val Loss: 1.327726\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.327726)\nEpoch 223 | Train Loss: 2.107256 | Val Loss: 1.344856\nEpoch 224 | Train Loss: 2.105504 | Val Loss: 1.354407\nEpoch 225 | Train Loss: 2.099913 | Val Loss: 1.317222\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.317222)\nEpoch 226 | Train Loss: 2.091815 | Val Loss: 1.356029\nEpoch 227 | Train Loss: 2.088316 | Val Loss: 1.308255\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.308255)\nEpoch 228 | Train Loss: 2.082256 | Val Loss: 1.351856\nEpoch 229 | Train Loss: 2.088533 | Val Loss: 1.351852\nEpoch 230 | Train Loss: 2.077492 | Val Loss: 1.387766\nEpoch 231 | Train Loss: 2.074254 | Val Loss: 1.349150\nEpoch 232 | Train Loss: 2.067495 | Val Loss: 1.346277\nEpoch 233 | Train Loss: 2.064630 | Val Loss: 1.321222\nEpoch 234 | Train Loss: 2.058659 | Val Loss: 1.330443\nEpoch 235 | Train Loss: 2.056984 | Val Loss: 1.392851\nEpoch 236 | Train Loss: 2.054080 | Val Loss: 1.351062\nEpoch 237 | Train Loss: 2.045824 | Val Loss: 1.292328\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.292328)\nEpoch 238 | Train Loss: 2.040272 | Val Loss: 1.360522\nEpoch 239 | Train Loss: 2.039395 | Val Loss: 1.282087\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.282087)\nEpoch 240 | Train Loss: 2.034036 | Val Loss: 1.282673\nEpoch 241 | Train Loss: 2.028963 | Val Loss: 1.323107\nEpoch 242 | Train Loss: 2.026668 | Val Loss: 1.328634\nEpoch 243 | Train Loss: 2.020360 | Val Loss: 1.306564\nEpoch 244 | Train Loss: 2.018785 | Val Loss: 1.271091\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.271091)\nEpoch 245 | Train Loss: 2.013919 | Val Loss: 1.316166\nEpoch 246 | Train Loss: 2.011056 | Val Loss: 1.278395\nEpoch 247 | Train Loss: 2.008394 | Val Loss: 1.268954\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.268954)\nEpoch 248 | Train Loss: 2.005943 | Val Loss: 1.311813\nEpoch 249 | Train Loss: 1.995204 | Val Loss: 1.293767\nEpoch 250 | Train Loss: 2.000547 | Val Loss: 1.255664\n  → Saved new best model to /kaggle/working/best_model.pth (Val Loss 1.255664)\n","output_type":"stream"}],"execution_count":31},{"id":"78c062d0","cell_type":"code","source":"plt.plot(train_losses, label='Train loss')\nplt.plot(val_losses, label='Validation loss')\nplt.xlabel('epochs')\nplt.ylabel('MSE Loss')\nplt.legend()\nplt.savefig('/kaggle/working/loss_curve.png',dpi=300)\nplt.show()","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:33:25.393305Z","iopub.execute_input":"2025-11-28T11:33:25.393557Z","iopub.status.idle":"2025-11-28T11:33:25.736596Z","shell.execute_reply.started":"2025-11-28T11:33:25.393532Z","shell.execute_reply":"2025-11-28T11:33:25.735922Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAkQAAAGwCAYAAABIC3rIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABKTklEQVR4nO3deXwU9eH/8fdsLhJzgUCOGm7kMqACxogi1kiglGrBCogCgvBFg/0Jiki9EFtp8ShFqfZrv5pSDxAVD2iBcFeMgGgQAanYSLAkoBwJRyDJ7vz+SDLsQoAEsrMD+3o+HvsgO/PZmc8MIXnzucYwTdMUAABAEHMFugIAAACBRiACAABBj0AEAACCHoEIAAAEPQIRAAAIegQiAAAQ9AhEAAAg6IUGugLnA4/Ho127dikmJkaGYQS6OgAAoBZM09TBgweVnJwsl+v0bUAEolrYtWuXUlJSAl0NAABwFnbu3KlLLrnktGUIRLUQExMjqfKGxsbGBrg2AACgNkpKSpSSkmL9Hj8dAlEtVHeTxcbGEogAADjP1Ga4C4OqAQBA0CMQAQCAoEcgAgAAQY8xRAAA23k8HpWVlQW6GrgAhIeHn3FKfW0QiAAAtiorK1N+fr48Hk+gq4ILgMvlUsuWLRUeHn5OxyEQAQBsY5qmCgsLFRISopSUlHr5nz2CV/XCyYWFhWrWrNk5LZ5MIAIA2KaiokJHjhxRcnKyoqKiAl0dXACaNGmiXbt2qaKiQmFhYWd9HKI5AMA2brdbks65ewOoVv29VP29dbYIRAAA2/FcSNSX+vpeIhABAICgRyACAABBj0AEAEAAtGjRQjNmzAj4MVCJQBRAbo+pPQeP6tsfDgW6KgCAUzAM47SvKVOmnNVx169frzFjxtRvZXHWmHYfQP/dX6qez6xQgzCXvn6qb6CrAwCoQWFhofX13Llz9fjjj2vbtm3WtujoaOtr0zTldrsVGnrmX69NmjSp34rinNBCFEANL6pcL+FouUelZec2XRAAzkemaepIWUVAXqZp1qqOiYmJ1isuLk6GYVjvv/76a8XExOif//ynunbtqoiICH388cf69ttvdfPNNyshIUHR0dHq3r27li5d6nPcE7u7DMPQX//6V/3yl79UVFSU2rZtqw8//LBO97OgoEA333yzoqOjFRsbq9tuu027d++29m/cuFE33HCDYmJiFBsbq65du+qzzz6TJO3YsUP9+/dXw4YNddFFF6lTp076xz/+Uafzn89oIQqg6IhQhboMVXhM7T9SpsjwyEBXCQBsVVruVsfHFwfk3FumZioqvH5+DT788MN69tln1apVKzVs2FA7d+7Uz372M/3ud79TRESEZs+erf79+2vbtm1q1qzZKY/z5JNPavr06XrmmWf0wgsvaOjQodqxY4caNWp0xjp4PB4rDK1atUoVFRXKysrSoEGDtHLlSknS0KFDdcUVV+ill15SSEiI8vLyrMUMs7KyVFZWptWrV+uiiy7Sli1bfFq/LnQBbSGaNm2aunfvrpiYGDVt2lS33HKLTzOkJPXq1euk/tqxY8f6lCkoKFC/fv0UFRWlpk2bauLEiaqoqPAps3LlSl155ZWKiIhQmzZtlJ2d7e/LOyPDMBQfVbmg1P4jPOQQAM5XU6dO1U033aTWrVurUaNG6tKli/7nf/5Hl112mdq2baunnnpKrVu3PmOLz4gRIzRkyBC1adNGTz/9tA4dOqR169bVqg7Lli3Tpk2b9Oabb6pr165KS0vT7NmztWrVKq1fv15S5e/LjIwMtW/fXm3bttWvfvUrdenSxdrXo0cPpaamqlWrVvr5z3+unj17ntuNOY8EtIVo1apVysrKUvfu3VVRUaHf/OY36t27t7Zs2aKLLrrIKjd69GhNnTrVeu+93Lvb7Va/fv2UmJioTz75RIWFhRo2bJjCwsL09NNPS5Ly8/PVr18/jR07Vm+88YaWLVumu+++W0lJScrMzLTvgmvQMCpMPx46pgNHygNaDwAIhMiwEG2ZGpifw5FhIfV2rG7duvm8P3TokKZMmaKFCxeqsLBQFRUVKi0tVUFBwWmP07lzZ+vriy66SLGxsdqzZ0+t6rB161alpKQoJSXF2taxY0fFx8dr69at6t69uyZMmKC7775bf//735WRkaFf/epXat26tSTp17/+te655x4tWbJEGRkZGjhwoE99LnQBbSFatGiRRowYoU6dOqlLly7Kzs5WQUGBNmzY4FMuKirKpw83NjbW2rdkyRJt2bJFr7/+ui6//HL17dtXTz31lGbNmqWysspWl5dfflktW7bUc889pw4dOmjcuHG69dZb9cc//tHW661JQ1qIAAQxwzAUFR4akFd9rpbt/Z94SXrwwQc1f/58Pf300/rXv/6lvLw8paamWr+XTuXEZ3EZhiGPx1Nv9ZwyZYo2b96sfv36afny5erYsaPmz58vSbr77rv1n//8R3feeac2bdqkbt266YUXXqi3czudowZVFxcXS9JJfaVvvPGGGjdurMsuu0yTJ0/WkSNHrH25ublKTU1VQkKCtS0zM1MlJSXavHmzVSYjI8PnmJmZmcrNza2xHseOHVNJSYnPy1+qB1bvp4UIAC4Ya9as0YgRI/TLX/5SqampSkxM1HfffefXc3bo0EE7d+7Uzp07rW1btmzRgQMH1LFjR2vbpZdeqvHjx2vJkiUaMGCAXnvtNWtfSkqKxo4dq/fee08PPPCAXnnlFb/W2UkcE4g8Ho/uv/9+9ejRQ5dddpm1/fbbb9frr7+uFStWaPLkyfr73/+uO+64w9pfVFTkE4YkWe+LiopOW6akpESlpaUn1WXatGmKi4uzXt7Nj/XNaiE6TAsRAFwo2rZtq/fee095eXnauHGjbr/99npt6alJRkaGUlNTNXToUH3++edat26dhg0bpuuvv17dunVTaWmpxo0bp5UrV2rHjh1as2aN1q9frw4dOkiS7r//fi1evFj5+fn6/PPPtWLFCmtfMHDMLLOsrCx99dVX+vjjj322ey9alZqaqqSkJN1444369ttvrX7P+jZ58mRNmDDBel9SUuK3UMSgagC48Dz//PMaOXKkrrnmGjVu3FiTJk3ya2+DVNm99sEHH+i+++5Tz5495XK51KdPH6vbKyQkRHv37tWwYcO0e/duNW7cWAMGDNCTTz4pqXJMblZWlr7//nvFxsaqT58+jhhaYhdHBKJx48ZpwYIFWr16tS655JLTlk1LS5Mkbd++Xa1bt1ZiYuJJI/Cr11xITEy0/vReh6G6TGxsrCIjT57qHhERoYiIiLO+nrpoGFXZZcagagBwvhEjRmjEiBHW+169etW4nlGLFi20fPlyn21ZWVk+70/sQqvpOAcOHDhtfU48RrNmzfTBBx/UWDY8PFxvvfXWKY8VTOOFahLQLjPTNDVu3DjNnz9fy5cvV8uWLc/4mby8PElSUlKSJCk9PV2bNm3yGYWfk5Oj2NhYq880PT1dy5Yt8zlOTk6O0tPT6+lKzlLpfl3z39c0IfRtWogAAAiggAairKwsvf7663rzzTcVExOjoqIiFRUVWeN6vv32Wz311FPasGGDvvvuO3344YcaNmyYevbsaU0F7N27tzp27Kg777xTGzdu1OLFi/Xoo48qKyvLauUZO3as/vOf/+ihhx7S119/rT//+c96++23NX78+IBduySp/KhS//2CskI+YAwRAAABFNBA9NJLL6m4uFi9evVSUlKS9Zo7d66kyua9pUuXqnfv3mrfvr0eeOABDRw4UB999JF1jJCQEC1YsEAhISFKT0/XHXfcoWHDhvmsW9SyZUstXLhQOTk56tKli5577jn99a9/DfgaRGoQJ0kKMUwdPezfvmUAAHBqAR1DdKbnyKSkpGjVqlVnPE7z5s3P+LyVXr166YsvvqhT/fwuLFKmK0yGp1zu0v2Brg0AAEHLMdPug5JhyIyoaiU6VqIKt3+nZAIAgJoRiALMiKxcdTtWR3SglJlmAAAEAoEowIyqcUQxxhEGVgMAECAEokCrCkSxOsLjOwAACBACUaBVByLjMGsRAcAFrFevXrr//vut9y1atNCMGTNO+xnDMPT++++f87nr6zinM2XKFF1++eV+PYc/EYgCzauF6ACBCAAcp3///urTp0+N+/71r3/JMAx9+eWXdT7u+vXrfR5PVR9OFUoKCwvVt2/fej3XhYZAFGgRlYOqYwy6zADAiUaNGqWcnBx9//33J+177bXX1K1bN2ux4Lpo0qSJoqKi6qOKZ5SYmGjbI6nOVwSiQGsQL6lqDBGDqgHAcX7+85+rSZMmys7O9tl+6NAhzZs3T6NGjdLevXs1ZMgQ/eQnP1FUVJRSU1NP+9ww6eQus2+++UY9e/ZUgwYN1LFjR+Xk5Jz0mUmTJunSSy9VVFSUWrVqpccee0zl5ZX/mc7OztaTTz6pjRs3yjAMGYZh1fnELrNNmzbppz/9qSIjI3XxxRdrzJgxOnTokLV/xIgRuuWWW/Tss88qKSlJF198sbKysqxz1YbH49HUqVN1ySWXKCIiQpdffrkWLVpk7S8rK9O4ceOUlJSkBg0aqHnz5po2bZqkynUKp0yZombNmikiIkLJycn69a9/Xetznw1HPNw1qDGGCEAwM02p/Ehgzh0WJRnGGYuFhoZq2LBhys7O1iOPPCKj6jPz5s2T2+3WkCFDdOjQIXXt2lWTJk1SbGysFi5cqDvvvFOtW7fWVVdddcZzeDweDRgwQAkJCVq7dq2Ki4t9xhtVi4mJUXZ2tpKTk7Vp0yaNHj1aMTExeuihhzRo0CB99dVXWrRokZYuXSpJiouLO+kYhw8fVmZmptLT07V+/Xrt2bNHd999t8aNG+cT+lasWKGkpCStWLFC27dv16BBg3T55Zdr9OjRZ7weSfrTn/6k5557Tn/5y190xRVX6NVXX9UvfvELbd68WW3bttXMmTP14Ycf6u2331azZs20c+dO7dy5U5L07rvv6o9//KPmzJmjTp06qaioSBs3bqzVec8WgSjQmGUGIJiVH5GeTg7MuX+zSwq/qFZFR44cqWeeeUarVq1Sr169JFV2lw0cOFBxcXGKi4vTgw8+aJW/7777tHjxYr399tu1CkRLly7V119/rcWLFys5ufJ+PP300yeN+3n00Uetr1u0aKEHH3xQc+bM0UMPPaTIyEhFR0crNDRUiYmJpzzXm2++qaNHj2r27Nm66KLK63/xxRfVv39//eEPf1BCQoIkqWHDhnrxxRcVEhKi9u3bq1+/flq2bFmtA9Gzzz6rSZMmafDgwZKkP/zhD1qxYoVmzJihWbNmqaCgQG3bttW1114rwzDUvHlz67MFBQVKTExURkaGwsLC1KxZs1rdx3NBl1mgWS1EDKoGAKdq3769rrnmGr366quSpO3bt+tf//qXRo0aJUlyu9166qmnlJqaqkaNGik6OlqLFy9WQUFBrY6/detWpaSkWGFIktLT008qN3fuXPXo0UOJiYmKjo7Wo48+WutzeJ+rS5cuVhiSpB49esjj8Wjbtm3Wtk6dOikkJMR6n5SUpD179tTqHCUlJdq1a5d69Ojhs71Hjx7aunWrpMpuuby8PLVr106//vWvtWTJEqvcr371K5WWlqpVq1YaPXq05s+fr4qKijpdZ13RQhRoDaoGVeuIDh717182ADhOWFRlS02gzl0Ho0aN0n333adZs2bptddeU+vWrXX99ddLkp555hn96U9/0owZM5SamqqLLrpI999/v8rK6u8/urm5uRo6dKiefPJJZWZmKi4uTnPmzNFzzz1Xb+fwFhYW5vPeMAx5PPX3iKkrr7xS+fn5+uc//6mlS5fqtttuU0ZGht555x2lpKRo27ZtWrp0qXJycnTvvfdaLXQn1qu+0EIUaF4tREfL3QGuDADYzDAqu60C8arF+CFvt912m1wul958803Nnj1bI0eOtMYTrVmzRjfffLPuuOMOdenSRa1atdK///3vWh+7Q4cO2rlzpwoLC61tn376qU+ZTz75RM2bN9cjjzyibt26qW3bttqxY4dPmfDwcLndp/9d0qFDB23cuFGHDx+2tq1Zs0Yul0vt2rWrdZ1PJzY2VsnJyVqzZo3P9jVr1qhjx44+5QYNGqRXXnlFc+fO1bvvvqt9+/ZJkiIjI9W/f3/NnDlTK1euVG5urjZt2lQv9asJLUSBZo0hOqzSMlqIAMCpoqOjNWjQIE2ePFklJSUaMWKEta9t27Z655139Mknn6hhw4Z6/vnntXv3bp9f/qeTkZGhSy+9VMOHD9czzzyjkpISPfLIIz5l2rZtq4KCAs2ZM0fdu3fXwoULNX/+fJ8yLVq0UH5+vvLy8nTJJZcoJibmpOn2Q4cO1RNPPKHhw4drypQp+uGHH3TffffpzjvvtMYP1YeJEyfqiSeeUOvWrXX55ZfrtddeU15ent544w1J0vPPP6+kpCRdccUVcrlcmjdvnhITExUfH6/s7Gy53W6lpaUpKipKr7/+uiIjI33GGdU3WogCrSoQhRtuecpLA1wZAMDpjBo1Svv371dmZqbPeJ9HH31UV155pTIzM9WrVy8lJibqlltuqfVxXS6X5s+fr9LSUl111VW6++679bvf/c6nzC9+8QuNHz9e48aN0+WXX65PPvlEjz32mE+ZgQMHqk+fPrrhhhvUpEmTGqf+R0VFafHixdq3b5+6d++uW2+9VTfeeKNefPHFut2MM/j1r3+tCRMm6IEHHlBqaqoWLVqkDz/8UG3btpVUOWNu+vTp6tatm7p3767vvvtO//jHP+RyuRQfH69XXnlFPXr0UOfOnbV06VJ99NFHuvjii+u1jt4M0zRNvx39AlFSUqK4uDgVFxcrNja2fg9umjKnNpJhenRN+Z/1ye+G1u/xAcBBjh49qvz8fLVs2VINGjQIdHVwATjd91Rdfn/TQhRohiGzarXqSM9hlbvrb8AaAACoHQKRE1R1m8XpMAOrAQAIAAKRAxheM81KCUQAANiOQOQA1YEoRkd0tIwuMwAA7EYgcgJaiAAEGebzoL7U1/cSgcgJvJ5nRiACcCGrfhREfa7gjOBW/b3k/ZiRs8HCjE7g9cT70jICEYALV2hoqKKiovTDDz8oLCxMLhf/L8fZ83g8+uGHHxQVFaXQ0HOLNAQiJ/BqIWKWGYALmWEYSkpKUn5+/kmPnQDOhsvlUrNmzazHqJwtApETVA+qZgwRgCAQHh6utm3b0m2GehEeHl4vLY0EIieoWpgxVke0ly4zAEHA5XKxUjUchc5bJ/AeQ0QLEQAAtiMQOUF4lCQpUmWMIQIAIAAIRE4QEi5JClc5s8wAAAgAApEThERIksJUQZcZAAABQCBygtCqFiKDQAQAQCAQiJzAq8uMMUQAANiPQOQEVYEoTG7GEAEAEAAEIicIrRxDFK5yuswAAAgAApETVLUQRRgVtBABABAABCInqApEklRediyAFQEAIDgRiJzAKxC5y48GsCIAAAQnApETVI0hkqQKWogAALAdgcgJXCEyjRBJkrucQAQAgN0IRA5hhoRJkjx0mQEAYDsCkVNUPb7DXVEW4IoAABB8CEQOYVYNrDYrjsk0zQDXBgCA4EIgcgij6nlmYWa5jlV4AlwbAACCC4HIIYzQ40+853lmAADYi0DkEEYIT7wHACBQCEROYT3xnsd3AABgNwKRU/CAVwAAAoZA5BReLUSMIQIAwF4EIqeoCkRhqlBpGbPMAACwE4HIKaq7zAy6zAAAsBuByCmqHt0RLmaZAQBgNwKRU4RUD6qu0FFmmQEAYCsCkVNYg6rpMgMAwG4EIqcI9RpUTSACAMBWBCKnqO4yM1iYEQAAuwU0EE2bNk3du3dXTEyMmjZtqltuuUXbtm3zKXP06FFlZWXp4osvVnR0tAYOHKjdu3f7lCkoKFC/fv0UFRWlpk2bauLEiaqoqPAps3LlSl155ZWKiIhQmzZtlJ2d7e/LqxtrUHU56xABAGCzgAaiVatWKSsrS59++qlycnJUXl6u3r176/Dhw1aZ8ePH66OPPtK8efO0atUq7dq1SwMGDLD2u91u9evXT2VlZfrkk0/0t7/9TdnZ2Xr88cetMvn5+erXr59uuOEG5eXl6f7779fdd9+txYsX23q9pxV6fFA1XWYAANjLME3TDHQlqv3www9q2rSpVq1apZ49e6q4uFhNmjTRm2++qVtvvVWS9PXXX6tDhw7Kzc3V1VdfrX/+85/6+c9/rl27dikhIUGS9PLLL2vSpEn64YcfFB4erkmTJmnhwoX66quvrHMNHjxYBw4c0KJFi85Yr5KSEsXFxam4uFixsbH+ufiVv5dWTtMbFTdq4+VPaPqtXfxzHgAAgkRdfn87agxRcXGxJKlRo0aSpA0bNqi8vFwZGRlWmfbt26tZs2bKzc2VJOXm5io1NdUKQ5KUmZmpkpISbd682SrjfYzqMtXHONGxY8dUUlLi8/I7r1lmFW7HZFQAAIKCYwKRx+PR/fffrx49euiyyy6TJBUVFSk8PFzx8fE+ZRMSElRUVGSV8Q5D1fur952uTElJiUpLS0+qy7Rp0xQXF2e9UlJS6uUaT6uqyyzMqFC5h0AEAICdHBOIsrKy9NVXX2nOnDmBroomT56s4uJi67Vz507/n9Tr4a5uD88yAwDATqGBroAkjRs3TgsWLNDq1at1ySWXWNsTExNVVlamAwcO+LQS7d69W4mJiVaZdevW+Ryvehaad5kTZ6bt3r1bsbGxioyMPKk+ERERioiIqJdrqzWvLrNyuswAALBVQFuITNPUuHHjNH/+fC1fvlwtW7b02d+1a1eFhYVp2bJl1rZt27apoKBA6enpkqT09HRt2rRJe/bsscrk5OQoNjZWHTt2tMp4H6O6TPUxHMFrllmFmxYiAADsFNAWoqysLL355pv64IMPFBMTY435iYuLU2RkpOLi4jRq1ChNmDBBjRo1UmxsrO677z6lp6fr6quvliT17t1bHTt21J133qnp06erqKhIjz76qLKysqxWnrFjx+rFF1/UQw89pJEjR2r58uV6++23tXDhwoBd+0mq1yEyKlTBGCIAAGwV0Bail156ScXFxerVq5eSkpKs19y5c60yf/zjH/Xzn/9cAwcOVM+ePZWYmKj33nvP2h8SEqIFCxYoJCRE6enpuuOOOzRs2DBNnTrVKtOyZUstXLhQOTk56tKli5577jn99a9/VWZmpq3Xe1pVK1WHqYJZZgAA2MxR6xA5lS3rEP17ifTmr7TJ00JTk1/SvLHX+Oc8AAAEifN2HaKgFnp8lhmDqgEAsBeByCm8u8yYdg8AgK0IRE5RPe3eYAwRAAB2IxA5RVWXWYTKmWUGAIDNCERO4TPLjC4zAADsRCByiup1iMQ6RAAA2I1A5BShrEMEAECgEIicompQdZjhlttdEeDKAAAQXAhETlEViCTJcJcFsCIAAAQfApFTVHWZSZLLpIUIAAA7EYicwhVmfWm4jwWwIgAABB8CkVO4XDKrQlGIpzzAlQEAILgQiJykahyRyywXz9wFAMA+BCIHMasf38Fq1QAA2IpA5CTW4ztYiwgAADsRiBzEqF6LSBUq54n3AADYhkDkJFVT78NVTgsRAAA2IhA5iFH9gFejQhW0EAEAYBsCkZN4P+CVFiIAAGxDIHISuswAAAgIApGTWNPu6TIDAMBOBCIn8QlEtBABAGAXApGTVHeZGeUqd9NCBACAXQhETlI1qDqMQdUAANiKQOQkIdWDqukyAwDATgQiJwn1epYZXWYAANiGQOQk1YOqDVqIAACwE4HISby6zBhUDQCAfQhEThJ6/OGublqIAACwDYHISUKOjyEqZ5YZAAC2IRA5ic8sM7rMAACwC4HISUJCJUmhcrMOEQAANiIQOYmrMhCFGB5mmQEAYCMCkZO4vFuI6DIDAMAuBCInMUIkSSFyq5wWIgAAbEMgchJXZSAKlUduWogAALANgchJqrrMXGIMEQAAdiIQOYnXGCLWIQIAwD4EIiepnmUmD4OqAQCwEYHISawxRG66zAAAsBGByEl81iGihQgAALsQiJzEu4WIMUQAANiGQOQkXmOIGFQNAIB9CEROYgUit9x0mQEAYBsCkZN4LczIStUAANiHQOQkXi1ETLsHAMA+BCIn8VmHiBYiAADsQiByEu+n3dNlBgCAbQhETlL9tHvWIQIAwFYEIiepGlTNtHsAAOxFIHISry4zN11mAADYhkDkJF6zzMqZZQYAgG0IRE5itRAxywwAADsRiJzEGkPkZlA1AAA2IhA5iddK1Uy7BwDAPgENRKtXr1b//v2VnJwswzD0/vvv++wfMWKEDMPwefXp08enzL59+zR06FDFxsYqPj5eo0aN0qFDh3zKfPnll7ruuuvUoEEDpaSkaPr06f6+tLNT1WXmossMAABbBTQQHT58WF26dNGsWbNOWaZPnz4qLCy0Xm+99ZbP/qFDh2rz5s3KycnRggULtHr1ao0ZM8baX1JSot69e6t58+basGGDnnnmGU2ZMkX/+7//67frOmtVgSjMcKu8wh3gygAAEDxCA3nyvn37qm/fvqctExERocTExBr3bd26VYsWLdL69evVrVs3SdILL7ygn/3sZ3r22WeVnJysN954Q2VlZXr11VcVHh6uTp06KS8vT88//7xPcHIE1/G/Do+HQAQAgF0cP4Zo5cqVatq0qdq1a6d77rlHe/futfbl5uYqPj7eCkOSlJGRIZfLpbVr11plevbsqfDwcKtMZmamtm3bpv3799d4zmPHjqmkpMTnZYuqMUSSJAIRAAC2qXMgKi0t1ZEjR6z3O3bs0IwZM7RkyZJ6rZhU2V02e/ZsLVu2TH/4wx+0atUq9e3bV253ZVgoKipS06ZNfT4TGhqqRo0aqaioyCqTkJDgU6b6fXWZE02bNk1xcXHWKyUlpb4vrWbeLUTucnvOCQAA6t5ldvPNN2vAgAEaO3asDhw4oLS0NIWFhenHH3/U888/r3vuuafeKjd48GDr69TUVHXu3FmtW7fWypUrdeONN9bbeU40efJkTZgwwXpfUlJiTyjyCkSmu8L/5wMAAJLOooXo888/13XXXSdJeuedd5SQkKAdO3Zo9uzZmjlzZr1X0FurVq3UuHFjbd++XZKUmJioPXv2+JSpqKjQvn37rHFHiYmJ2r17t0+Z6venGpsUERGh2NhYn5ctDO8uMwIRAAB2qXMgOnLkiGJiYiRJS5Ys0YABA+RyuXT11Vdrx44d9V5Bb99//7327t2rpKQkSVJ6eroOHDigDRs2WGWWL18uj8ejtLQ0q8zq1atVXn68CyonJ0ft2rVTw4YN/VrfOvMaQ8SgagAA7FPnQNSmTRu9//772rlzpxYvXqzevXtLkvbs2VPnlpRDhw4pLy9PeXl5kqT8/Hzl5eWpoKBAhw4d0sSJE/Xpp5/qu+++07Jly3TzzTerTZs2yszMlCR16NBBffr00ejRo7Vu3TqtWbNG48aN0+DBg5WcnCxJuv322xUeHq5Ro0Zp8+bNmjt3rv70pz/5dIk5hmHIrG4looUIAAD7mHU0b948MywszHS5XOZNN91kbX/66afNPn361OlYK1asMCWd9Bo+fLh55MgRs3fv3maTJk3MsLAws3nz5ubo0aPNoqIin2Ps3bvXHDJkiBkdHW3Gxsaad911l3nw4EGfMhs3bjSvvfZaMyIiwvzJT35i/v73v69TPYuLi01JZnFxcZ0+dzY8U5uY5hOx5vWPzPb7uQAAuJDV5fe3YZpmnZdELioqUmFhobp06SKXq7KRad26dYqNjVX79u3rL605RElJieLi4lRcXOz38USe3yXJVX5EN5TN0Iqn7/LruQAAuJDV5ff3WS3MmJiYaA1ILikp0fLly9WuXbsLMgzZrnqmmemWaZoyDCOw9QEAIAjUeQzRbbfdphdffFFS5ZpE3bp102233abOnTvr3XffrfcKBp2qQFT5xHueZwYAgB3qHIhWr15tTbufP3++TNPUgQMHNHPmTP32t7+t9woGG6NqUHUID3gFAMA2dQ5ExcXFatSokSRp0aJFGjhwoKKiotSvXz9988039V7BoFPVQhQqjyo8ngBXBgCA4FDnQJSSkqLc3FwdPnxYixYtsqbd79+/Xw0aNKj3CgadkOoWIjctRAAA2KTOg6rvv/9+DR06VNHR0WrevLl69eolqbIrLTU1tb7rF3QMq4XIrXJaiAAAsEWdA9G9996rq666Sjt37tRNN91kTbtv1aoVY4jqgzWomjFEAADY5aym3Xfr1k3dunWTaZrW1PB+/frVd92CU3ULkeGWm1lmAADYos5jiCRp9uzZSk1NVWRkpCIjI9W5c2f9/e9/r++6BSevFqJyN11mAADYoc4tRM8//7wee+wxjRs3Tj169JAkffzxxxo7dqx+/PFHjR8/vt4rGVSMyowaIg/rEAEAYJM6B6IXXnhBL730koYNG2Zt+8UvfqFOnTppypQpBKJz5bUwIy1EAADYo85dZoWFhbrmmmtO2n7NNdeosLCwXioV1LxmmTGGCAAAe9Q5ELVp00Zvv/32Sdvnzp2rtm3b1kulgprPGCICEQAAdqhzl9mTTz6pQYMGafXq1dYYojVr1mjZsmU1BiXUkatyYcZQuVVBlxkAALaocwvRwIEDtXbtWjVu3Fjvv/++3n//fTVu3Fjr1q3TL3/5S3/UMbh4tRDRZQYAgD3Oah2irl276vXXX/fZtmfPHj399NP6zW9+Uy8VC1reXWYEIgAAbHFW6xDVpLCwUI899lh9HS54VXWZhRh0mQEAYJd6C0SoJ15Pu2dQNQAA9iAQOY3r+NPuGUMEAIA9CERO47UOUQVPuwcAwBa1HlQ9YcKE0+7/4YcfzrkyEOsQAQAQALUORF988cUZy/Ts2fOcKgP5thAxqBoAAFvUOhCtWLHCn/VAtaoxRC4e7goAgG0YQ+Q0RvVK1R5aiAAAsAmByGmqxxAZblqIAACwCYHIaXxmmRGIAACwA4HIaax1iHiWGQAAdiEQOY3PLDMCEQAAdqh1IJo+fbpKS0ut92vWrNGxY8es9wcPHtS9995bv7ULRl7rELEwIwAA9qh1IJo8ebIOHjxove/bt6/++9//Wu+PHDmiv/zlL/Vbu2Dk1WXGGCIAAOxR60BkmuZp36OesDAjAAC2YwyR09BCBACA7QhETlPdQmTwtHsAAOxS60d3SNJf//pXRUdHS5IqKiqUnZ2txo0bS5LP+CKcA2tQtZuHuwIAYJNaB6JmzZrplVdesd4nJibq73//+0llcI68Zpm5mWUGAIAtah2IvvvuOz9WAxbvMUS0EAEAYAvGEDmN9XBXHt0BAIBdah2IcnNztWDBAp9ts2fPVsuWLdW0aVONGTPGZ6FGnCWfLjMCEQAAdqh1IJo6dao2b95svd+0aZNGjRqljIwMPfzww/roo480bdo0v1QyqHitQ1TOOkQAANii1oEoLy9PN954o/V+zpw5SktL0yuvvKIJEyZo5syZevvtt/1SyaDiNcuMFiIAAOxR60C0f/9+JSQkWO9XrVqlvn37Wu+7d++unTt31m/tglHVoOpQw6NyAhEAALaodSBKSEhQfn6+JKmsrEyff/65rr76amv/wYMHFRYWVv81DDZVLUQupt0DAGCbWgein/3sZ3r44Yf1r3/9S5MnT1ZUVJSuu+46a/+XX36p1q1b+6WSQcXnWWa0EAEAYIdar0P01FNPacCAAbr++usVHR2tv/3tbwoPD7f2v/rqq+rdu7dfKhlUeJYZAAC2q3Ugaty4sVavXq3i4mJFR0crJCTEZ/+8efOsx3rgHLhYhwgAALvV6VlmkhQXF1fj9kaNGp1zZSCfdYgqmHYPAIAtah2IRo4cWatyr7766llXBvIZQ8S0ewAA7FHrQJSdna3mzZvriiuukGnyi9pvvGaZ0WUGAIA9ah2I7rnnHr311lvKz8/XXXfdpTvuuINuMn/wmWVGlxkAAHao9bT7WbNmqbCwUA899JA++ugjpaSk6LbbbtPixYtpMapP1bPMDFqIAACwS52edh8REaEhQ4YoJydHW7ZsUadOnXTvvfeqRYsWOnTokL/qGFy8n3bPOkQAANiiToHI54MulwzDkGmacrvd9Vmn4OY9y4wWIgAAbFGnQHTs2DG99dZbuummm3TppZdq06ZNevHFF1VQUMAaRPXFZ5YZY4gAALBDrQdV33vvvZozZ45SUlI0cuRIvfXWW2rcuLE/6xacvFeqpssMAABb1DoQvfzyy2rWrJlatWqlVatWadWqVTWWe++99+qtckGJLjMAAGxX6y6zYcOG6YYbblB8fLzi4uJO+aqL1atXq3///kpOTpZhGHr//fd99pumqccff1xJSUmKjIxURkaGvvnmG58y+/bt09ChQxUbG6v4+HiNGjXqpAHeX375pa677jo1aNBAKSkpmj59ep3qaSsrELlVQZcZAAC2qNPCjPXt8OHD6tKli0aOHKkBAwactH/69OmaOXOm/va3v6lly5Z67LHHlJmZqS1btqhBgwaSpKFDh6qwsFA5OTkqLy/XXXfdpTFjxujNN9+UJJWUlKh3797KyMjQyy+/rE2bNmnkyJGKj4/XmDFj6v2azpn3OkS0EAEAYIs6P8usPvXt21d9+/atcZ9pmpoxY4YeffRR3XzzzZKk2bNnKyEhQe+//74GDx6srVu3atGiRVq/fr26desmSXrhhRf0s5/9TM8++6ySk5P1xhtvqKysTK+++qrCw8PVqVMn5eXl6fnnn3doIKpeh8iUTI88HlMulxHgSgEAcGE762n3/pafn6+ioiJlZGRY2+Li4pSWlqbc3FxJUm5uruLj460wJEkZGRlyuVxau3atVaZnz54KDw+3ymRmZmrbtm3av39/jec+duyYSkpKfF62qQpEUuU4onK6zQAA8DvHBqKioiJJUkJCgs/2hIQEa19RUZGaNm3qsz80NFSNGjXyKVPTMbzPcaJp06b5jItKSUk59wuqLdfxRrsQeXjAKwAANnBsIAqkyZMnq7i42Hrt3LnTvpOfEIjKmXoPAIDfOTYQJSYmSpJ2797ts3337t3WvsTERO3Zs8dnf0VFhfbt2+dTpqZjeJ/jRBEREYqNjfV52cYrEFUuzkggAgDA3xwbiFq2bKnExEQtW7bM2lZSUqK1a9cqPT1dkpSenq4DBw5ow4YNVpnly5fL4/EoLS3NKrN69WqVl5dbZXJyctSuXTs1bNjQpqupA8N7DBFT7wEAsENAA9GhQ4eUl5envLw8SZUDqfPy8lRQUCDDMHT//ffrt7/9rT788ENt2rRJw4YNU3Jysm655RZJUocOHdSnTx+NHj1a69at05o1azRu3DgNHjxYycnJkqTbb79d4eHhGjVqlDZv3qy5c+fqT3/6kyZMmBCgqz4Dl0syKv9aQlmtGgAAWwR02v1nn32mG264wXpfHVKGDx+u7OxsPfTQQzp8+LDGjBmjAwcO6Nprr9WiRYusNYgk6Y033tC4ceN04403yuVyaeDAgZo5c6a1Py4uTkuWLFFWVpa6du2qxo0b6/HHH3fmlPtqRohkehRClxkAALYwTNPkN+4ZlJSUKC4uTsXFxfaMJ/ptolRRqmuP/UmzJ9yqVk14cC4AAHVVl9/fjh1DFNS8Ht9BCxEAAP5HIHIi7yfeE4gAAPA7ApETeT/xnkHVAAD4HYHIiXwe8Mq0ewAA/I1A5ERWlxlPvAcAwA4EIieqCkSsQwQAgD0IRE7ELDMAAGxFIHIir0HV5YwhAgDA7whETlQdiAyP3HSZAQDgdwQiJ7LGEDHLDAAAOxCInMhrDBGzzAAA8D8CkRMZzDIDAMBOBCInooUIAABbEYicyFqp2iM3Y4gAAPA7ApETeT3ctZwuMwAA/I5A5EQszAgAgK0IRE5U3WVmuFXupssMAAB/IxA5kddK1bQQAQDgfwQiJ3JV/rWEMssMAABbEIicyBUmqSoQMagaAAC/IxA5UUi4pMpAxLR7AAD8j0DkRCGVLUThqlA5XWYAAPgdgciJqlqIwo1yBlUDAGADApETVQWiMFUwhggAABsQiJyoqsssTG5VMIYIAAC/IxA5UWiEpKoWIrrMAADwOwKRE1WPIVKFKlipGgAAvyMQOZHVZUYLEQAAdiAQOVH1oGqjgllmAADYgEDkRMwyAwDAVgQiJ/JamJFZZgAA+B+ByIl8BlXTQgQAgL8RiJzIu8uMMUQAAPgdgciJqmeZMagaAABbEIicKKRyYcZwVaicdYgAAPA7ApETea1DRAsRAAD+RyByIq8xROUEIgAA/I5A5ERegcjNtHsAAPyOQORE1qBqN9PuAQCwAYHIibzXIaLLDAAAvyMQOZFPlxmBCAAAfyMQOZH16I5ypt0DAGADApET0UIEAICtCEROFFq1MKPhVgUtRAAA+B2ByImquswkSe6ywNUDAIAgQSByoqouM0kyzIoAVgQAgOBAIHIir0DkcpcHsCIAAAQHApETuUJkGpV/NS4PgQgAAH8jEDlVVSuRy8MYIgAA/I1A5FCmq3JgdYgq5GHqPQAAfkUgcioe3wEAgG0IRE4V6h2IWIsIAAB/IhA5lOG1WjUtRAAA+BeByKm8H9/hJhABAOBPBCKHslqIjAqV02UGAIBfOToQTZkyRYZh+Lzat29v7T969KiysrJ08cUXKzo6WgMHDtTu3bt9jlFQUKB+/fopKipKTZs21cSJE1VRcR6s/mw98Z4HvAIA4G+hga7AmXTq1ElLly613oeGHq/y+PHjtXDhQs2bN09xcXEaN26cBgwYoDVr1kiS3G63+vXrp8TERH3yyScqLCzUsGHDFBYWpqefftr2a6kT7zFEdJkBAOBXjg9EoaGhSkxMPGl7cXGx/u///k9vvvmmfvrTn0qSXnvtNXXo0EGffvqprr76ai1ZskRbtmzR0qVLlZCQoMsvv1xPPfWUJk2apClTpig8PPyk4zoGg6oBALCNo7vMJOmbb75RcnKyWrVqpaFDh6qgoECStGHDBpWXlysjI8Mq2759ezVr1ky5ubmSpNzcXKWmpiohIcEqk5mZqZKSEm3evPmU5zx27JhKSkp8Xrar6jILU4XcjCECAMCvHB2I0tLSlJ2drUWLFumll15Sfn6+rrvuOh08eFBFRUUKDw9XfHy8z2cSEhJUVFQkSSoqKvIJQ9X7q/edyrRp0xQXF2e9UlJS6vfCaqN6YUajQuV0mQEA4FeO7jLr27ev9XXnzp2Vlpam5s2b6+2331ZkZKTfzjt58mRNmDDBel9SUmJ/KPJamJFB1QAA+JejW4hOFB8fr0svvVTbt29XYmKiysrKdODAAZ8yu3fvtsYcJSYmnjTrrPp9TeOSqkVERCg2NtbnZTuvMUTlbrrMAADwp/MqEB06dEjffvutkpKS1LVrV4WFhWnZsmXW/m3btqmgoEDp6emSpPT0dG3atEl79uyxyuTk5Cg2NlYdO3a0vf51wqBqAABs4+guswcffFD9+/dX8+bNtWvXLj3xxBMKCQnRkCFDFBcXp1GjRmnChAlq1KiRYmNjdd999yk9PV1XX321JKl3797q2LGj7rzzTk2fPl1FRUV69NFHlZWVpYiIiABf3Rl4Dao+Wu4OcGUAALiwOToQff/99xoyZIj27t2rJk2a6Nprr9Wnn36qJk2aSJL++Mc/yuVyaeDAgTp27JgyMzP15z//2fp8SEiIFixYoHvuuUfp6em66KKLNHz4cE2dOjVQl1R7XoOqj5QRiAAA8CdHB6I5c+acdn+DBg00a9YszZo165Rlmjdvrn/84x/1XTX/8+oyKyUQAQDgV+fVGKKgYnWZuVVKlxkAAH5FIHIqrxYiuswAAPAvApFThVQO+o5QuUrLzoOH0QIAcB4jEDmV1ywzWogAAPAvApFTVXeZMcsMAAC/IxA5FesQAQBgGwKRU1WvQyQ3LUQAAPgZgcipmGUGAIBtCERO5dVlVlrOLDMAAPyJQORUDKoGAMA2BCKnssYQ8egOAAD8jUDkVKFegYhZZgAA+BWByKkYVA0AgG0IRE7F0+4BALANgcipqmeZGRU6UlYh0zQDXCEAAC5cBCKn8hpU7TGlMrcnwBUCAODCRSByKq8uM0l0mwEA4EcEIqfyWphREgOrAQDwIwKRU1V3mRkEIgAA/I1A5FRWl5lbkkmXGQAAfkQgcqqqQCSxOCMAAP5GIHIqr0BUuTgjD3gFAMBfCEROVTWoWmJxRgAA/I1A5FSuEMkIkcTjOwAA8DcCkZN5Lc54hDFEAAD4DYHIyapnmhkVOkoLEQAAfkMgcjJrcUY3XWYAAPgRgcjJfLrMmGUGAIC/EIicLPT488yYZQYAgP8QiJwsPFqSFGscpssMAAA/IhA5WXSCJKmpcYCVqgEA8CMCkZPFJEqSmuoAXWYAAPgRgcjJoptKkpoYB3h0BwAAfkQgcrLoyhaiJgYtRAAA+BOByMlijo8hYlA1AAD+QyBysmivMUQMqgYAwG8IRE5WNYaoqXFApccYQwQAgL8QiJysapZZlHFMRvmhAFcGAIALF4HIycIvkqdqccaYin3yeMwAVwgAgAsTgcjpvMYRHTxKtxkAAP5AIHI4lzXTbL+2/3AwwLUBAODCRCByOq/Hd3xdRCACAMAfCEROF3N8ccZtBCIAAPyCQOR0Xi1EBCIAAPyDQOR0VYGoiQ5o2+6DMk1mmgEAUN8IRE5XNag6wTigA0fKtefgsQBXCACACw+ByOmqpt0nuooliYHVAAD4AYHI6aoGVcfqoCJ1VNuKSgJcIQAALjwEIqeLbCjFpUiS0lxbta2IR3gAAFDfCEROZxhSmxslSde7vtTmXcUBrhAAABceAtH5oM1NkqQbQjbq66KD2lpItxkAAPWJQHQ+aHW95ApVC6NIzY0ivbm2INA1AgDggkIgOh9ExEjN0iVJ17s26v0v/qsjZTzoFQCA+kIgOl+0yZAk/SLiCx08Vq53P/9vgCsEAMCFg0B0vmjfTzJc6ub5UneELNWUDzfrf1d/qwq3J9A1AwDgvBdUgWjWrFlq0aKFGjRooLS0NK1bty7QVaq9xm2ljCmSpCfDZus2Y6le+MfnSnt6mSa986XeWlegDTv26fv9R1RWQUgCAKAuDDNIHo41d+5cDRs2TC+//LLS0tI0Y8YMzZs3T9u2bVPTpk1P+9mSkhLFxcWpuLhYsbGxNtW4BqYpvTtK+updSVK5QvSN5xJtN5O1z4xRsaK134zWATNaZoN4hUZEKiI8XBEREWoQEaHIiHCFRETJDI+RER6p8NAQhYaEKSwsVKFhYQoLDVd4WIjCQ10KD3UpzOWSyyWFGIZCXIZcLuP411V/hrgkl2EotLpsVRmX95/W18ePZRhG4O4jACAo1OX3d9AEorS0NHXv3l0vvviiJMnj8SglJUX33XefHn744dN+1jGBSJLKj0q5L0pfvi39uK3eD19huuSWSx5V/2l4fV353tpvVr43q16SrK9Nn69P3Kaq8sc/5zFc1uclQ6ZRvV+SDMkwZMp1/L1Xmcp9Vceq3lb9WeOEbV5fV3/uVGUM4+RjVn9tVu/z3l8d8gxX1TW6vD4r33LWsV1e+10+5czqOlSX8a6nIck6vo4fx+c6VPVndUOwy+ueVX2+6j5VnseoLFP93jr/8fqYhiHD+rO6vj5/nLDNsN6aXiHY8C59wueNqu8D79B8/Eujxs9Ulj/hnEbV9pOOUfNxTjrnCSc3TthuGsYJx6w6d03HkE76T4BRdewatkou33OetP+Eup1aDftrqsfpjnXKc5yijjWUN+t6jFNd5ymOU9NW02trjX/3Jx7DqLnDxPT6fjrdMYxTH7rq33qNZz3h3amv0/t7/FTHONNlVv/brVlNx6/5Out2jFNURjrpOkNDw9ShXftTHPvs1OX3d2i9ntmhysrKtGHDBk2ePNna5nK5lJGRodzc3JPKHzt2TMeOHX+IakmJg9b9CWsg9XxQuu4B6UCBtHuztO8/Uul+qXS/zCP7VH54r9yH98ksL5PpKZfprpA85ZKnQqHuowp3H5FLNXerhRoehZ5i30nO9LO4PtUU24MiygNAcPhBDaUp3wXs/EERiH788Ue53W4lJCT4bE9ISNDXX399Uvlp06bpySeftKt6Z8cwpIbNK1/emyWFn+mzpil5KiTTI3nckumufO+p/rPqa9PjW8anfNU+0115PJk+f5qmRx7TlMdjyvS45fGYcpsemR6PTNOUp6qMaZoyPR7JNOUxq/70mJXbq/ebpmR6fN5XHkfWNln7qt+bx8tWX4fXtsrykqnj5eVzvurPqbJeMmVUHaPyHnpkyqwKZZXljapyNd0PWWU9p9xvnOr9acpY26uOX9ne4/H6bNU284Q2Oq/3xz9X1c7mfcwa9h//7Cm+t2reUbvt5im2V/E+r/epTqyPaW0/03lOV7cz1MU81Tnrdg+qj3Pi3lP/f+PM96Z25Wt24nWd7jinOmf91b1mp6pjvd2DczqOWcO22hyjlnWsw7+R0x7nrOtz+uOf/H1c++8btyuixu12CYpAVFeTJ0/WhAkTrPclJSVKSUkJYI3qmWFIIWH+PYWkkKoXAABnkhjg8wdFIGrcuLFCQkK0e/dun+27d+9WYuLJfwURERGKiAhsUgUAAPYJimn34eHh6tq1q5YtW2Zt83g8WrZsmdLT0wNYMwAA4ARB0UIkSRMmTNDw4cPVrVs3XXXVVZoxY4YOHz6su+66K9BVAwAAARY0gWjQoEH64Ycf9Pjjj6uoqEiXX365Fi1adNJAawAAEHyCZh2ic+GodYgAAECt1OX3d1CMIQIAADgdAhEAAAh6BCIAABD0CEQAACDoEYgAAEDQIxABAICgRyACAABBj0AEAACCHoEIAAAEvaB5dMe5qF7Mu6SkJMA1AQAAtVX9e7s2D+UgENXCwYMHJUkpKSkBrgkAAKirgwcPKi4u7rRleJZZLXg8Hu3atUsxMTEyDKNej11SUqKUlBTt3LmT56T5EffZPtxr+3Cv7cF9tk9932vTNHXw4EElJyfL5Tr9KCFaiGrB5XLpkksu8es5YmNj+YdmA+6zfbjX9uFe24P7bJ/6vNdnahmqxqBqAAAQ9AhEAAAg6BGIAiwiIkJPPPGEIiIiAl2VCxr32T7ca/twr+3BfbZPIO81g6oBAEDQo4UIAAAEPQIRAAAIegQiAAAQ9AhEAAAg6BGIAmjWrFlq0aKFGjRooLS0NK1bty7QVTrvTZkyRYZh+Lzat29v7T969KiysrJ08cUXKzo6WgMHDtTu3bsDWOPzw+rVq9W/f38lJyfLMAy9//77PvtN09Tjjz+upKQkRUZGKiMjQ998841PmX379mno0KGKjY1VfHy8Ro0apUOHDtl4FeeHM93rESNGnPQ93qdPH58y3OszmzZtmrp3766YmBg1bdpUt9xyi7Zt2+ZTpjY/LwoKCtSvXz9FRUWpadOmmjhxoioqKuy8FMerzb3u1avXSd/XY8eO9Snj73tNIAqQuXPnasKECXriiSf0+eefq0uXLsrMzNSePXsCXbXzXqdOnVRYWGi9Pv74Y2vf+PHj9dFHH2nevHlatWqVdu3apQEDBgSwtueHw4cPq0uXLpo1a1aN+6dPn66ZM2fq5Zdf1tq1a3XRRRcpMzNTR48etcoMHTpUmzdvVk5OjhYsWKDVq1drzJgxdl3CeeNM91qS+vTp4/M9/tZbb/ns516f2apVq5SVlaVPP/1UOTk5Ki8vV+/evXX48GGrzJl+XrjdbvXr109lZWX65JNP9Le//U3Z2dl6/PHHA3FJjlWbey1Jo0eP9vm+nj59urXPlnttIiCuuuoqMysry3rvdrvN5ORkc9q0aQGs1fnviSeeMLt06VLjvgMHDphhYWHmvHnzrG1bt241JZm5ubk21fD8J8mcP3++9d7j8ZiJiYnmM888Y207cOCAGRERYb711lumaZrmli1bTEnm+vXrrTL//Oc/TcMwzP/+97+21f18c+K9Nk3THD58uHnzzTef8jPc67OzZ88eU5K5atUq0zRr9/PiH//4h+lyucyioiKrzEsvvWTGxsaax44ds/cCziMn3mvTNM3rr7/e/H//7/+d8jN23GtaiAKgrKxMGzZsUEZGhrXN5XIpIyNDubm5AazZheGbb75RcnKyWrVqpaFDh6qgoECStGHDBpWXl/vc9/bt26tZs2bc93OQn5+voqIin/saFxentLQ0677m5uYqPj5e3bp1s8pkZGTI5XJp7dq1ttf5fLdy5Uo1bdpU7dq10z333KO9e/da+7jXZ6e4uFiS1KhRI0m1+3mRm5ur1NRUJSQkWGUyMzNVUlKizZs321j788uJ97raG2+8ocaNG+uyyy7T5MmTdeTIEWufHfeah7sGwI8//ii32+3zFytJCQkJ+vrrrwNUqwtDWlqasrOz1a5dOxUWFurJJ5/Uddddp6+++kpFRUUKDw9XfHy8z2cSEhJUVFQUmApfAKrvXU3fz9X7ioqK1LRpU5/9oaGhatSoEfe+jvr06aMBAwaoZcuW+vbbb/Wb3/xGffv2VW5urkJCQrjXZ8Hj8ej+++9Xjx49dNlll0lSrX5eFBUV1fh9X70PJ6vpXkvS7bffrubNmys5OVlffvmlJk2apG3btum9996TZM+9JhDhgtK3b1/r686dOystLU3NmzfX22+/rcjIyADWDKgfgwcPtr5OTU1V586d1bp1a61cuVI33nhjAGt2/srKytJXX33lM94Q/nGqe+09xi01NVVJSUm68cYb9e2336p169a21I0uswBo3LixQkJCTpqtsHv3biUmJgaoVhem+Ph4XXrppdq+fbsSExNVVlamAwcO+JThvp+b6nt3uu/nxMTEkyYMVFRUaN++fdz7c9SqVSs1btxY27dvl8S9rqtx48ZpwYIFWrFihS655BJre21+XiQmJtb4fV+9D75Oda9rkpaWJkk+39f+vtcEogAIDw9X165dtWzZMmubx+PRsmXLlJ6eHsCaXXgOHTqkb7/9VklJSeratavCwsJ87vu2bdtUUFDAfT8HLVu2VGJios99LSkp0dq1a637mp6ergMHDmjDhg1WmeXLl8vj8Vg/+HB2vv/+e+3du1dJSUmSuNe1ZZqmxo0bp/nz52v58uVq2bKlz/7a/LxIT0/Xpk2bfAJoTk6OYmNj1bFjR3su5Dxwpntdk7y8PEny+b72+72ul6HZqLM5c+aYERERZnZ2trllyxZzzJgxZnx8vM8IetTdAw88YK5cudLMz88316xZY2ZkZJiNGzc29+zZY5qmaY4dO9Zs1qyZuXz5cvOzzz4z09PTzfT09ADX2vkOHjxofvHFF+YXX3xhSjKff/5584svvjB37NhhmqZp/v73vzfj4+PNDz74wPzyyy/Nm2++2WzZsqVZWlpqHaNPnz7mFVdcYa5du9b8+OOPzbZt25pDhgwJ1CU51unu9cGDB80HH3zQzM3NNfPz882lS5eaV155pdm2bVvz6NGj1jG412d2zz33mHFxcebKlSvNwsJC63XkyBGrzJl+XlRUVJiXXXaZ2bt3bzMvL89ctGiR2aRJE3Py5MmBuCTHOtO93r59uzl16lTzs88+M/Pz880PPvjAbNWqldmzZ0/rGHbcawJRAL3wwgtms2bNzPDwcPOqq64yP/3000BX6bw3aNAgMykpyQwPDzd/8pOfmIMGDTK3b99u7S8tLTXvvfdes2HDhmZUVJT5y1/+0iwsLAxgjc8PK1asMCWd9Bo+fLhpmpVT7x977DEzISHBjIiIMG+88UZz27ZtPsfYu3evOWTIEDM6OtqMjY0177rrLvPgwYMBuBpnO929PnLkiNm7d2+zSZMmZlhYmNm8eXNz9OjRJ/1Hint9ZjXdY0nma6+9ZpWpzc+L7777zuzbt68ZGRlpNm7c2HzggQfM8vJym6/G2c50rwsKCsyePXuajRo1MiMiIsw2bdqYEydONIuLi32O4+97bVRVFgAAIGgxhggAAAQ9AhEAAAh6BCIAABD0CEQAACDoEYgAAEDQIxABAICgRyACAABBj0AEAACCHoEIAGph5cqVMgzjpId9ArgwEIgAAEDQIxABAICgRyACcF7weDyaNm2aWrZsqcjISHXp0kXvvPOOpOPdWQsXLlTnzp3VoEEDXX311frqq698jvHuu++qU6dOioiIUIsWLfTcc8/57D927JgmTZqklJQURUREqE2bNvq///s/nzIbNmxQt27dFBUVpWuuuUbbtm2z9m3cuFE33HCDYmJiFBsbq65du+qzzz7z0x0BUJ8IRADOC9OmTdPs2bP18ssva/PmzRo/frzuuOMOrVq1yiozceJEPffcc1q/fr2aNGmi/v37q7y8XFJlkLnttts0ePBgbdq0SVOmTNFjjz2m7Oxs6/PDhg3TW2+9pZkzZ2rr1q36y1/+oujoaJ96PPLII3ruuef02WefKTQ0VCNHjrT2DR06VJdcconWr1+vDRs26OGHH1ZYWJh/bwyA+mECgMMdPXrUjIqKMj/55BOf7aNGjTKHDBlirlixwpRkzpkzx9q3d+9eMzIy0pw7d65pmqZ5++23mzfddJPP5ydOnGh27NjRNE3T3LZtmynJzMnJqbEO1edYunSptW3hwoWmJLO0tNQ0TdOMiYkxs7Ozz/2CAdiOFiIAjrd9+3YdOXJEN910k6Kjo63X7Nmz9e2331rl0tPTra8bNWqkdu3aaevWrZKkrVu3qkePHj7H7dGjh7755hu53W7l5eUpJCRE119//Wnr0rlzZ+vrpKQkSdKePXskSRMmTNDdd9+tjIwM/f73v/epGwBnIxABcLxDhw5JkhYuXKi8vDzrtWXLFmsc0bmKjIysVTnvLjDDMCRVjm+SpClTpmjz5s3q16+fli9fro4dO2r+/Pn1Uj8A/kUgAuB4HTt2VEREhAoKCtSmTRufV0pKilXu008/tb7ev3+//v3vf6tDhw6SpA4dOmjNmjU+x12zZo0uvfRShYSEKDU1VR6Px2dM0tm49NJLNX78eC1ZskQDBgzQa6+9dk7HA2CP0EBXAADOJCYmRg8++KDGjx8vj8eja6+9VsXFxVqzZo1iY2PVvHlzSdLUqVN18cUXKyEhQY888ogaN26sW265RZL0wAMPqHv37nrqqac0aNAg5ebm6sUXX9Sf//xnSVKLFi00fPhwjRw5UjNnzlSXLl20Y8cO7dmzR7fddtsZ61haWqqJEyfq1ltvVcuWLfX9999r/fr1GjhwoN/uC4B6FOhBTABQGx6Px5wxY4bZrl07MywszGzSpImZmZlprlq1yhrw/NFHH5mdOnUyw8PDzauuusrcuHGjzzHeeecds2PHjmZYWJjZrFkz85lnnvHZX1paao4fP95MSkoyw8PDzTZt2pivvvqqaZrHB1Xv37/fKv/FF1+Yksz8/Hzz2LFj5uDBg82UlBQzPDzcTE5ONseNG2cNuAbgbIZpmmaAMxkAnJOVK1fqhhtu0P79+xUfHx/o6gA4DzGGCAAABD0CEQAACHp0mQEAgKBHCxEAAAh6BCIAABD0CEQAACDoEYgAAEDQIxABAICgRyACAABBj0AEAACCHoEIAAAEvf8PBkxXeaFSIsAAAAAASUVORK5CYII=\n"},"metadata":{}}],"execution_count":32},{"id":"9802c827","cell_type":"code","source":"","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true},"outputs":[],"execution_count":null},{"id":"88be26bd-5637-45f8-b947-f7b2d3917109","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}